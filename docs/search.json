[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Series de Tiempo",
    "section": "",
    "text": "Prefacio\nNotas computacionales de clase para el curso CA0415, Series de Tiempo.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "box_jenk.html",
    "href": "box_jenk.html",
    "title": "1  Enfoque de Box-Jenkins",
    "section": "",
    "text": "1.1 Ejemplos de series temporales\nSerie de tiempo de Johnson & Johnson\njj_tibble &lt;- tk_tbl(jj)\njj_tibble %&gt;% \n  ggplot(aes(x = index, y = value)) +\n  geom_line() +\n  labs(x = \"Tiempo\", y = \"Ganancia\")+\n  theme_bw()\nSerie de tiempo de temperaturas globales\ntemp_tibble &lt;- tk_tbl(gtemp_both)\ntemp_tibble %&gt;% \n  ggplot(aes(x = index, y = value)) +\n  geom_line() +\n  labs(x = \"Tiempo\", y = \"Anomalías de Temp.\")+\n  theme_bw()\nSerie de tiempo del índice de Dow Jones:\ndjia_tibble &lt;- tk_tbl(djia)\ndjia_tibble %&gt;% \n  select(index,Close) %&gt;%\n  ggplot(aes(x = index, y = Close)) +\n  geom_line() +\n  labs(x = \"Tiempo\", y = \"Precio de Cierre\")+\n  theme_bw()\nY después de calcular los log-retornos:\ndjiar &lt;- diff(log(djia$Close))[-1]\ndjiar_tibble &lt;- tk_tbl(djiar)\ndjiar_tibble  %&gt;% \n  select(index,Close) %&gt;%\n  ggplot(aes(x = index, y = Close)) +\n  geom_line() +\n  labs(x = \"Tiempo\", y = \"Precio de Cierre\")+\n  theme_bw()\nSerie de tiempo del Niño y la población de peces:\nsoi_tibble &lt;- tk_tbl(soi) %&gt;% rename(soi = value)\n\nsoi_tsibble &lt;- soi_tibble %&gt;% mutate(index = yearmonth(index)) %&gt;%\n  as_tsibble(index = index)\n\nrec_tibble &lt;- tk_tbl(rec) %&gt;% rename(rec = value)\n\nrec_tsibble &lt;- rec_tibble %&gt;% mutate(index = yearmonth(index)) %&gt;%\n  as_tsibble(index = index)\n\nsoi_tsibble %&gt;% \n  ggplot(aes(x = index, y = soi)) +\n  geom_line() +\n  labs(x = \"Tiempo\", y = \"SOI\")+\n  theme_bw()\n\n\n\n\n\n\n\nrec_tsibble %&gt;% \n  ggplot(aes(x = index, y = rec)) +\n  geom_line() +\n  labs(x = \"Tiempo\", y = \"REC\")+\n  theme_bw()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Enfoque de Box-Jenkins</span>"
    ]
  },
  {
    "objectID": "box_jenk.html#gráficos-de-la-función-de-autocorrelación-estimada.",
    "href": "box_jenk.html#gráficos-de-la-función-de-autocorrelación-estimada.",
    "title": "1  Enfoque de Box-Jenkins",
    "section": "1.2 Gráficos de la función de autocorrelación estimada.",
    "text": "1.2 Gráficos de la función de autocorrelación estimada.\nSiguiendo este último ejemplo, vamos a calcular el ACF de ambas series de SOI y REC:\n\nACF_soi &lt;- soi_tsibble %&gt;% ACF(soi,lag_max = 24)\n\nsoi_tsibble %&gt;% ACF(soi,lag_max = 24) %&gt;%\n  autoplot() +\n  theme_bw()+labs(x = \"Lag\", y = \"ACF\")\n\n\n\n\n\n\n\nrec_tsibble %&gt;% ACF(rec,lag_max = 24) %&gt;%\n  autoplot() +\n  theme_bw()+labs(x = \"Lag\", y = \"ACF\")\n\n\n\n\n\n\n\n\nNoten que en la primera línea del codigo anterior se extrae directamente el ACF de la serie SOI. A continuación el CCF de ambas series:\n\nsoi_rec_tsibble &lt;- soi_tsibble %&gt;% left_join(rec_tsibble,by = \"index\")\n\nsoi_rec_tsibble %&gt;% CCF(x = soi, y = rec, lag_max = 24) %&gt;%\n  autoplot() +\n  theme_bw() +\n  labs(x = \"Lag\", y = \"CCF\")",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Enfoque de Box-Jenkins</span>"
    ]
  },
  {
    "objectID": "box_jenk.html#análisis-exploratorio-de-series-de-tiempo",
    "href": "box_jenk.html#análisis-exploratorio-de-series-de-tiempo",
    "title": "1  Enfoque de Box-Jenkins",
    "section": "1.3 Análisis exploratorio de series de tiempo",
    "text": "1.3 Análisis exploratorio de series de tiempo\n\n1.3.1 Descomposición de componente trigonométrico:\n\nset.seed(1492)\nnum &lt;- 120\nt &lt;- 1:num\n\nX &lt;- ts(2*cos(2*pi*t/12) + rnorm(120), frequency = 12)\nY &lt;- ts(2*cos(2*pi*(t+5)/12) + rnorm(120), frequency = 12)\n\n\nYw &lt;- resid(lm(Y ~ cos(2*pi*t/12) + sin(2*pi*t/12)))\n\n\nX_df &lt;- data.frame(\n  index = time(X),\n  value = as.numeric(X),\n  series = \"X\"\n)\n\nYw_df &lt;- data.frame(\n  index = time(Y),\n  value = as.numeric(Yw),\n  series = \"Yw\"\n)\n\n\ncombined_df &lt;- bind_rows(X_df, Yw_df)\n\n\ncombined_tsibble &lt;- as_tsibble(combined_df, index = index, key = series)\n\n\nggplot(combined_tsibble, aes(x = index, y = value)) +\n  geom_line() +\n  facet_wrap(~series, scales = \"free_y\") +  # Facet plot with free y-axis scales\n  labs(title = \"Time Series Plot\",\n       x = \"Time\",\n       y = \"Value\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nCon lo cual se compara una de las dos series originales con los residuos después de haberle ajustado un componente triginométrico.\n\n\n1.3.2 Descomposición de tendencia a través de regresión lineal\n\nchicken_tibble &lt;- tk_tbl(chicken)\n\nchicken_tsibble &lt;- as_tsibble(chicken, index = time(chicken)) %&gt;%\n  rename(chicken = value)\n\nfit_pollo &lt;- chicken_tsibble %&gt;%\n  model(lm = TSLM(chicken ~ trend()))\n\nmodel_summary &lt;- glance(fit_pollo)\nprint(model_summary)\n\n# A tibble: 1 × 15\n  .model r_squared adj_r_squared sigma2 statistic  p_value    df log_lik   AIC\n  &lt;chr&gt;      &lt;dbl&gt;         &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 lm         0.917         0.917   22.1     1974. 2.83e-98     2   -533.  561.\n# ℹ 6 more variables: AICc &lt;dbl&gt;, BIC &lt;dbl&gt;, CV &lt;dbl&gt;, deviance &lt;dbl&gt;,\n#   df.residual &lt;int&gt;, rank &lt;int&gt;\n\nmodel_coefficients &lt;- tidy(fit_pollo)\nprint(model_coefficients)\n\n# A tibble: 2 × 6\n  .model term        estimate std.error statistic   p.value\n  &lt;chr&gt;  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 lm     (Intercept)   58.6     0.703        83.3 1.54e-144\n2 lm     trend()        0.299   0.00674      44.4 2.83e- 98\n\nresiduals_df &lt;- augment(fit_pollo) %&gt;%\n  select(index, .resid) %&gt;%\n  rename(residuals = .resid)\n\n\nggplot(residuals_df, aes(x = index, y = residuals)) +\n  geom_line() +\n  labs(title = \"Residuals of the Linear Model\",\n       x = \"Time\",\n       y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\nacf_residuals &lt;- acf(residuals_df$residuals, plot = FALSE)\n\nacf_df &lt;- data.frame(\n  lag = acf_residuals$lag,\n  acf = acf_residuals$acf\n)\n\nggplot(acf_df, aes(x = lag, y = acf)) +\n  geom_bar(stat = \"identity\") +\n  labs(title = \"ACF of Residuals\",\n       x = \"Lag\",\n       y = \"ACF\") +\n  theme_minimal()",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Enfoque de Box-Jenkins</span>"
    ]
  },
  {
    "objectID": "cap4.html",
    "href": "cap4.html",
    "title": "2  Capítulo 4",
    "section": "",
    "text": "2.1 Decomposing a Non-Sinusoidal Cycle Using Regression",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Capítulo 4</span>"
    ]
  },
  {
    "objectID": "cap4.html#decomposing-a-non-sinusoidal-cycle-using-regression",
    "href": "cap4.html#decomposing-a-non-sinusoidal-cycle-using-regression",
    "title": "2  Capítulo 4",
    "section": "",
    "text": "2.1.1 Introduction\nThis example demonstrates how to decompose a simple dataset into its frequency components using regression. The data given complete one cycle but not in a sinusoidal way, so the first frequency component \\(\\omega_1 = 1/5\\) is expected to be large, while the second component \\(\\omega_2 = 2/5\\) is expected to be small.\n\n\n2.1.2 Data and Frequency Components\nThe dataset ( x = {1, 2, 3, 2, 1} ) is examined with the two frequency components. We use cosines and sines at the relevant frequencies to perform the decomposition.\n\n# Define the dataset\nx = c(1, 2, 3, 2, 1)\n\n# Define cosine and sine terms for the first and second frequency components\nc1 = cos(2 * pi * 1:5 * 1/5)\ns1 = sin(2 * pi * 1:5 * 1/5)\nc2 = cos(2 * pi * 1:5 * 2/5)\ns2 = sin(2 * pi * 1:5 * 2/5)\n\n# Combine the components into matrices for regression\nomega1 = cbind(c1, s1)\nomega2 = cbind(c2, s2)\nanova(lm(x~omega1+omega2))\n\nWarning in anova.lm(lm(x ~ omega1 + omega2)): ANOVA F-tests on an essentially\nperfect fit are unreliable\n\n\nAnalysis of Variance Table\n\nResponse: x\n          Df  Sum Sq Mean Sq F value Pr(&gt;F)\nomega1     2 2.74164 1.37082     NaN    NaN\nomega2     2 0.05836 0.02918     NaN    NaN\nResiduals  0 0.00000     NaN               \n\n\nThis analysis presents the periodograms of the SOI and Recruitment series. It explores the significance of certain periodicities, particularly the yearly cycle and a potential four-year El Niño cycle. Confidence intervals for these spectral peaks are also calculated, but the results show wide intervals, making it difficult to assert the significance of the four-year cycle. The periodograms for the SOI and Recruitment series show: - A narrow-band peak at the yearly cycle ( \\(\\omega = 1/12\\) ). - A wide-band peak centered around the four-year cycle ( \\(\\omega = 1/48\\) ), possibly linked to El Niño.\n\n### R Code to Reproduce the Periodogram\n# Load the astsa package\nlibrary(astsa)\n\n# Set up plotting parameters\npar(mfrow=c(2,1))\n\n# Compute and plot the periodogram for the SOI series\nsoi.per = mvspec(soi, log=\"no\")\nabline(v=1/4, lty=2) # Add a vertical line at the four-year cycle\n\n# Compute and plot the periodogram for the Recruitment series\nrec.per = mvspec(rec, log=\"no\")\nabline(v=1/4, lty=2)\n\n\n\n\n\n\n\n\nConfidence Intervals for Spectral Estimates\nWe compute approximate 95% confidence intervals for the spectrum at the yearly cycle \\(\\omega=1/12\\), and the possible four-year cycle \\(\\omega = 1/48\\).\n\n# Confidence interval boundaries\nU = qchisq(.025, 2)  # 0.05063\nL = qchisq(.975, 2)  # 7.37775\n\n# SOI periodogram values at specific frequencies\nsoi_per_1_12 = soi.per$spec[40] # SOI periodogram at freq 1/12 = 40/480\nsoi_per_1_48 = soi.per$spec[10] # SOI periodogram at freq 1/48 = 10/480\n\n# Confidence intervals for the yearly cycle\nCI_1_12 = c(2 * soi_per_1_12 / L, 2 * soi_per_1_12 / U) \nCI_1_12 # Approximate 95% CI for the yearly cycle\n\n[1]  0.2635573 38.4010800\n\n# Confidence intervals for the four-year cycle\nCI_1_48 = c(2 * soi_per_1_48 / L, 2 * soi_per_1_48 / U) \nCI_1_48 # Approximate 95% CI for the four-year cycle\n\n[1] 0.0145653 2.1222066\n\n\nInterpretation\n\nThe periodogram at the yearly cycle \\(\\omega = 1/12\\) is significant, with a 95% confidence interval that suggests its importance in the SOI series.\nThe wide confidence interval at \\(\\omega= 1/48\\) (representing the four-year cycle) indicates that this peak may not be significant.\n\nConclusion\nThe periodogram analysis suggests the presence of a strong yearly cycle and a possible but irregular four-year cycle, which could be linked to El Niño. Further analysis is required to refine these findings.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Capítulo 4</span>"
    ]
  },
  {
    "objectID": "cap4.html#smoothing-the-periodogram",
    "href": "cap4.html#smoothing-the-periodogram",
    "title": "2  Capítulo 4",
    "section": "2.2 Smoothing the Periodogram",
    "text": "2.2 Smoothing the Periodogram\nThis example shows the smoothed periodogram for the SOI and Recruitment series, using a Daniell kernel to average the periodograms computed earlier. The goal is to reduce noise in the spectrum while maintaining key features, particularly the El Niño frequency. The smoothed spectra help in identifying the predominant periods and their significance.\n\n2.2.1 Averaged Periodogram Calculation\nThe Daniell kernel is used with ( L = 9 ) to compute the averaged periodograms. This provides a balance between noise reduction and retaining important peaks, as shown in the R code below.\n\n# Compute and plot the averaged periodogram for SOI\nsoi.ave = mvspec(soi, kernel('daniell', 4), log='no')\n\nBandwidth: 0.225 \nDegrees of Freedom: 16.99 \n\nabline(v=c(.25, 1, 2, 3), lty=2) # Add vertical lines at key frequencies\n\n\n\n\n\n\n\n# Display bandwidth of the SOI periodogram\nsoi.ave$bandwidth\n\n[1] 0.225\n\n# Bandwidth is 0.225, adjusted for the frequency scale in cycles per year\n\n# Compute and plot the averaged periodogram for Recruitment series\nrec.ave = mvspec(rec, kernel('daniell', 4), log='no')\n\nBandwidth: 0.225 \nDegrees of Freedom: 16.99 \n\nabline(v=c(.25, 1, 2, 3), lty=2) # Add vertical lines at key frequencies\n\n\n\n\n\n\n\n\nConfidence Intervals for Spectral Peaks\nWe compute 95% confidence intervals for the SOI spectrum at key frequencies, such as the El Niño cycle (48 months) and the yearly cycle.\n\n# Degrees of freedom for the averaged periodogram\ndf = soi.ave$df\ndf # Returned value: 16.9875\n\n[1] 16.9875\n\n# Compute chi-squared limits\nU = qchisq(.025, df)  # Upper limit\nL = qchisq(.975, df)  # Lower limit\n\n# Spectrum values at key frequencies\nsoi_spec_1_48 = soi.ave$spec[10]  # Spectrum at frequency 1/48\nsoi_spec_1_12 = soi.ave$spec[40]  # Spectrum at frequency 1/12\n\n# Confidence intervals for the 48-month (El Niño) cycle\nCI_1_48 = c(df * soi_spec_1_48 / L, df * soi_spec_1_48 / U)\nCI_1_48 # Approximate 95% confidence interval for 1/48\n\n[1] 0.02787891 0.11133335\n\n# Confidence intervals for the yearly cycle (1/12)\nCI_1_12 = c(df * soi_spec_1_12 / L, df * soi_spec_1_12 / U)\nCI_1_12 # Approximate 95% confidence interval for 1/12\n\n[1] 0.06703963 0.26772011\n\n\nInterpretation\nThe smoothed spectra highlight the El Niño frequency and the yearly cycle more clearly:\n\nAt \\(\\omega=1/12\\) (yearly cycle), the smoothing slightly flattens and spreads the peak. Harmonics of the yearly cycle appear at frequencies like \\(\\omega=1\\Delta,2\\Delta,\\cdots\\).\nConfidence intervals suggest that the El Niño frequency (48 months) shows significant power, with lower limits exceeding baseline spectral levels, confirming its importance in the SOI and Recruitment series.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Capítulo 4</span>"
    ]
  },
  {
    "objectID": "cap4.html#introduction-1",
    "href": "cap4.html#introduction-1",
    "title": "2  Capítulo 4",
    "section": "2.3 Introduction",
    "text": "2.3 Introduction\nThis example shows how to estimate the spectra of the SOI and Recruitment series using a smoothed periodogram. A modified Daniell kernel with ( m = 3 ) is applied, and the periodogram is smoothed twice. The resulting estimates are considered more visually appealing than those in previous examples.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Capítulo 4</span>"
    ]
  },
  {
    "objectID": "cap4.html#smoothing-the-periodogram-with-a-modified-daniell-kernel",
    "href": "cap4.html#smoothing-the-periodogram-with-a-modified-daniell-kernel",
    "title": "2  Capítulo 4",
    "section": "2.4 Smoothing the Periodogram with a Modified Daniell Kernel",
    "text": "2.4 Smoothing the Periodogram with a Modified Daniell Kernel\nWe use the Daniell kernel, smoothed with ( m = 3 ), to estimate the spectra. The kernel coefficients are shown below.\n\n# Define and plot the modified Daniell kernel with m = 3\nk = kernel(\"modified.daniell\", c(3, 3))\n\n# Display the kernel coefficients\nk$coef\n\n[1] 0.152777778 0.138888889 0.111111111 0.083333333 0.055555556 0.027777778\n[7] 0.006944444\n\n# Plot the kernel\nplot(k)\n\n\n\n\n\n\n\n\nEstimating the Spectra for the SOI Series\nWe now apply the smoothed kernel to the SOI series, plot the periodogram, and calculate the bandwidth and degrees of freedom.\n\n# Calculate and plot the smoothed periodogram for the SOI series\nsoi.smo = mvspec(soi, kernel=k, taper=.1, log=\"no\")\n\nBandwidth: 0.231 \nDegrees of Freedom: 15.61 \n\nabline(v=c(.25, 1), lty=2) # Add vertical lines at key frequencies\n\n\n\n\n\n\n\n# Retrieve degrees of freedom and bandwidth\ndf_soi = soi.smo$df\nbandwidth_soi = soi.smo$bandwidth\n\nsoi.smo$spec[soi.smo$freq==1]\n\n[1] 0.1675368\n\ndf_soi*soi.smo$spec[soi.smo$freq==1]/qchisq(0.975,df = df_soi)\n\n[1] 0.09235481\n\ndf_soi*soi.smo$spec[soi.smo$freq==1]/qchisq(0.025,df = df_soi)\n\n[1] 0.3929982\n\n# Display degrees of freedom and bandwidth\ndf_soi  # Degrees of freedom: 17.42618\n\n[1] 15.61029\n\nbandwidth_soi  # Bandwidth: 0.2308103\n\n[1] 0.2308103\n\n\nEstimating the Spectra for the Recruitment Series\nWe repeat the above steps for the Recruitment series.\n\n# Calculate and plot the smoothed periodogram for the Recruitment series\nrec.smo = mvspec(rec, kernel=k, taper=.1, log=\"no\")\n\nBandwidth: 0.231 \nDegrees of Freedom: 15.61 \n\nabline(v=c(.25, 1), lty=2) # Add vertical lines at key frequencies\n\n\n\n\n\n\n\n# Retrieve degrees of freedom and bandwidth\ndf_rec = rec.smo$df\nbandwidth_rec = rec.smo$bandwidth\n\n# Display degrees of freedom and bandwidth\ndf_rec  # Degrees of freedom: same as SOI\n\n[1] 15.61029\n\nbandwidth_rec  # Bandwidth: same as SOI\n\n[1] 0.2308103",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Capítulo 4</span>"
    ]
  },
  {
    "objectID": "cap4.html#smoothed-periodogram-for-soi-and-recruitment-series",
    "href": "cap4.html#smoothed-periodogram-for-soi-and-recruitment-series",
    "title": "2  Capítulo 4",
    "section": "2.5 Smoothed Periodogram for SOI and Recruitment Series",
    "text": "2.5 Smoothed Periodogram for SOI and Recruitment Series\n\n2.5.1 Introduction\nThis example shows how to estimate the spectra of the SOI and Recruitment series using a smoothed periodogram with a modified Daniell kernel. The kernel is smoothed twice with ( m = 3 ), yielding ( \\(L = 2m + 1 = 7\\) ). The bandwidth and degrees of freedom are calculated, and a taper of 10% is applied to reduce leakage effects.\n\n\n2.5.2 Defining the Modified Daniell Kernel\nThe modified Daniell kernel is created and its coefficients are displayed. This kernel is used to smooth the periodogram.\n\n# Define and plot the modified Daniell kernel with m = 3\nk = kernel(\"modified.daniell\", c(3, 3))\n\n# Display the kernel coefficients\nk$coef\n\n[1] 0.152777778 0.138888889 0.111111111 0.083333333 0.055555556 0.027777778\n[7] 0.006944444\n\n# Plot the kernel\nplot(k)\n\n\n\n\n\n\n\n\n\n\n2.5.3 Smoothed Spectral Estimate for SOI\nThe smoothed periodogram for the SOI series is calculated, applying a 10% taper to reduce spectral leakage. The degrees of freedom and bandwidth are then retrieved.\n\n# Calculate and plot the smoothed periodogram for the SOI series\nsoi.smo = mvspec(soi, kernel=k, taper=.1, log=\"no\")\n\nBandwidth: 0.231 \nDegrees of Freedom: 15.61 \n\nabline(v=c(.25, 1), lty=2) # Add vertical lines at key frequencies\n\n\n\n\n\n\n\n# Retrieve degrees of freedom and bandwidth\ndf_soi = soi.smo$df\nbandwidth_soi = soi.smo$bandwidth\n\n# Display degrees of freedom and bandwidth\ndf_soi  # Degrees of freedom: 17.42618\n\n[1] 15.61029\n\nbandwidth_soi  # Bandwidth: 0.2308103\n\n[1] 0.2308103\n\n\n\n\n2.5.4 Smoothed Spectral Estimate for Recruitment\nWe repeat the steps for the Recruitment series.\n\n# Calculate and plot the smoothed periodogram for the Recruitment series\nrec.smo = mvspec(rec, kernel=k, taper=.1, log=\"no\")\n\nBandwidth: 0.231 \nDegrees of Freedom: 15.61 \n\nabline(v=c(.25, 1), lty=2) # Add vertical lines at key frequencies\n\n\n\n\n\n\n\n# Retrieve degrees of freedom and bandwidth\ndf_rec = rec.smo$df\nbandwidth_rec = rec.smo$bandwidth\n\n# Display degrees of freedom and bandwidth\ndf_rec  # Degrees of freedom: same as SOI\n\n[1] 15.61029\n\nbandwidth_rec  # Bandwidth: same as SOI\n\n[1] 0.2308103\n\n\n\n\n2.5.5 Alternative Method for Estimation\nAn alternative way to generate the smoothed periodogram is by using the spans argument instead of explicitly defining the Daniell kernel. The spans vector specifies the smoothing parameter in terms of \\(L=2m+1\\), where \\(m=3\\).\n\n# Alternative method using spans to define the smoothing parameter\nsoi.smo_alt = mvspec(soi, taper=.1, spans=c(7, 7))\n\nBandwidth: 0.231 \nDegrees of Freedom: 15.61",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Capítulo 4</span>"
    ]
  },
  {
    "objectID": "cap4.html#the-effect-of-tapering-on-the-soi-series",
    "href": "cap4.html#the-effect-of-tapering-on-the-soi-series",
    "title": "2  Capítulo 4",
    "section": "2.6 The Effect of Tapering on the SOI Series",
    "text": "2.6 The Effect of Tapering on the SOI Series\n\n2.6.1 Introduction\nIn this example, we examine how tapering affects the spectral estimate of the SOI series. Tapering helps mitigate the effect of spectral leakage, which can blur periodic signals in the data. We compare the spectrum with no tapering against a spectrum with full tapering (50%). The fully tapered spectrum better distinguishes between the yearly cycle ($= 1 \\() and the El Niño cycle (\\) = 1/4 $).\n\n\n2.6.2 Spectral Estimation with and without Tapering\nWe calculate the spectral estimate of the SOI series twice: once with no tapering and once with full tapering (50%).\n\n# Calculate the spectrum with no tapering\ns0 = mvspec(soi, spans=c(7, 7), plot=FALSE) # No taper\n\n# Calculate the spectrum with full tapering (50%)\ns50 = mvspec(soi, spans=c(7, 7), taper=.5, plot=FALSE) # Full taper\n\n\n\n2.6.3 Plotting the Spectral Estimates\nWe now plot the spectral estimates on a log scale, using a solid line for the fully tapered spectrum and a dashed line for the non-tapered spectrum.\n\n# Plot the fully tapered spectrum (solid line)\nplot(s50$freq, s50$spec, log=\"y\", type=\"l\", ylab=\"Spectrum\", xlab=\"Frequency\")\n\n# Add the non-tapered spectrum (dashed line)\nlines(s0$freq, s0$spec, lty=2) # Dashed line for no taper\n\n\n\n\n\n\n\n\n\n\n2.6.4 Interpretation\nFrom the plot, we observe that tapering (solid line) leads to better separation between the yearly cycle (\\(\\omega=1\\)) and the El Niño cycle (\\(\\omega=1/4\\)). The non-tapered spectrum (dashed line) shows more spectral leakage, blurring these two important periodic components. By applying a taper, we reduce this leakage and obtain a clearer picture of the underlying spectral features.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Capítulo 4</span>"
    ]
  },
  {
    "objectID": "cap5.html",
    "href": "cap5.html",
    "title": "3  Capítulo 5",
    "section": "",
    "text": "3.1 Analysis of U.S. GNP",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Capítulo 5</span>"
    ]
  },
  {
    "objectID": "cap5.html#analysis-of-u.s.-gnp",
    "href": "cap5.html#analysis-of-u.s.-gnp",
    "title": "3  Capítulo 5",
    "section": "",
    "text": "3.1.1 Background\nIn Example 3.39, we fit an MA(2) model and an AR(1) model to the U.S. GNP series. The residuals from both models appeared to resemble a white noise process, but Example 3.43 suggested that the AR(1) model might be a better fit. It has been proposed that the U.S. GNP series may exhibit ARCH (Autoregressive Conditional Heteroskedasticity) errors, so we will investigate this possibility in this example.\nIf the GNP noise term follows an ARCH process, the squared residuals from the model should behave like a non-Gaussian AR(1) process, as noted in Equation 5.39.\n\n3.1.1.1 Initial Analysis of Residuals\nThe following R code generates the ACF and PACF plots of the squared residuals to check for remaining dependence:\n\nlibrary(astsa)\n# Fit AR(1) model to differenced log-transformed GNP\nu &lt;- sarima(diff(log(gnp)), 1, 0, 0)\n\ninitial  value -4.589567 \niter   2 value -4.654150\niter   3 value -4.654150\niter   4 value -4.654151\niter   4 value -4.654151\niter   4 value -4.654151\nfinal  value -4.654151 \nconverged\ninitial  value -4.655919 \niter   2 value -4.655921\niter   3 value -4.655922\niter   4 value -4.655922\niter   5 value -4.655922\niter   5 value -4.655922\niter   5 value -4.655922\nfinal  value -4.655922 \nconverged\n&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;&lt;&gt;\n \nCoefficients: \n      Estimate     SE t.value p.value\nar1     0.3467 0.0627  5.5255       0\nxmean   0.0083 0.0010  8.5398       0\n\nsigma^2 estimated as 9.029569e-05 on 220 degrees of freedom \n \nAIC = -6.44694  AICc = -6.446693  BIC = -6.400958 \n \n\n\n\n\n\n\n\n\nacf2(resid(u$fit)^2, 20)  # ACF and PACF of squared residuals\n\n\n\n\n\n\n\n\n     [,1] [,2] [,3] [,4]  [,5] [,6]  [,7] [,8] [,9] [,10] [,11] [,12] [,13]\nACF  0.12 0.13 0.03 0.13  0.01 0.05 -0.03 0.06 0.08 -0.08  0.09  0.10  0.01\nPACF 0.12 0.12 0.00 0.12 -0.02 0.02 -0.04 0.05 0.08 -0.12  0.11  0.09 -0.05\n     [,14] [,15] [,16] [,17] [,18] [,19] [,20]\nACF   0.04  0.15 -0.02  0.04 -0.05  0.01  0.05\nPACF  0.05  0.13 -0.09  0.01 -0.05  0.00  0.04\n\n\nThe plots reveal that there may be some small remaining dependence in the squared residuals.\n\n\n3.1.1.2 Fitting an AR(1)-ARCH(1) Model\nTo examine the ARCH nature of the residuals, we fit an AR(1)-ARCH(1) model to the GNP returns using the fGarch package. The garchFit function models the AR(1) and ARCH(1) components. The code and a summary of the results are shown below:\n\nlibrary(fGarch)\n\nNOTE: Packages 'fBasics', 'timeDate', and 'timeSeries' are no longer\nattached to the search() path when 'fGarch' is attached.\n\nIf needed attach them yourself in your R script by e.g.,\n        require(\"timeSeries\")\n\n# Fit AR(1)-ARCH(1) model\nfit &lt;- garchFit(~ arma(1, 0) + garch(1, 0), data = diff(log(gnp)))\n\n\nSeries Initialization:\n ARMA Model:                arma\n Formula Mean:              ~ arma(1, 0)\n GARCH Model:               garch\n Formula Variance:          ~ garch(1, 0)\n ARMA Order:                1 0\n Max ARMA Order:            1\n GARCH Order:               1 0\n Max GARCH Order:           1\n Maximum Order:             1\n Conditional Dist:          norm\n h.start:                   2\n llh.start:                 1\n Length of Series:          222\n Recursion Init:            mci\n Series Scale:              0.01015924\n\nParameter Initialization:\n Initial Parameters:          $params\n Limits of Transformations:   $U, $V\n Which Parameters are Fixed?  $includes\n Parameter Matrix:\n                     U          V    params includes\n    mu     -8.20681904   8.206819 0.8205354     TRUE\n    ar1    -0.99999999   1.000000 0.3466459     TRUE\n    omega   0.00000100 100.000000 0.1000000     TRUE\n    alpha1  0.00000001   1.000000 0.1000000     TRUE\n    gamma1 -0.99999999   1.000000 0.1000000    FALSE\n    delta   0.00000000   2.000000 2.0000000    FALSE\n    skew    0.10000000  10.000000 1.0000000    FALSE\n    shape   1.00000000  10.000000 4.0000000    FALSE\n Index List of Parameters to be Optimized:\n    mu    ar1  omega alpha1 \n     1      2      3      4 \n Persistence:                  0.1 \n\n\n--- START OF TRACE ---\nSelected Algorithm: nlminb \n\nR coded nlminb Solver: \n\n  0:     682.89527: 0.820535 0.346646 0.100000 0.100000\n  1:     308.43148: 0.763492 0.258112  1.06104 0.352453\n  2:     306.07332: 0.681276 0.195897  1.04763 0.304072\n  3:     301.00807: 0.561958 0.448458 0.825277 0.0402737\n  4:     298.88361: 0.383716 0.465477 0.632947 0.385969\n  5:     296.74288: 0.504144 0.389445 0.683635 0.247795\n  6:     296.67703: 0.497724 0.366843 0.688130 0.229496\n  7:     296.60039: 0.500011 0.385702 0.703145 0.211105\n  8:     296.59692: 0.515646 0.374174 0.690079 0.194961\n  9:     296.56381: 0.513570 0.367018 0.702272 0.200013\n 10:     296.55723: 0.523440 0.363126 0.708406 0.194151\n 11:     296.55632: 0.522578 0.364913 0.710104 0.194839\n 12:     296.55598: 0.520871 0.364956 0.710924 0.193212\n 13:     296.55568: 0.519486 0.366571 0.710212 0.194511\n 14:     296.55568: 0.519509 0.366597 0.710266 0.194512\n 15:     296.55568: 0.519511 0.366585 0.710290 0.194451\n 16:     296.55568: 0.519505 0.366562 0.710299 0.194464\n 17:     296.55568: 0.519526 0.366560 0.710295 0.194472\n 18:     296.55568: 0.519522 0.366563 0.710295 0.194471\n\nFinal Estimate of the Negative LLH:\n LLH:  -722.2849    norm LLH:  -3.253536 \n          mu          ar1        omega       alpha1 \n0.0052779470 0.3665625656 0.0000733096 0.1944713341 \n\nR-optimhess Difference Approximated Hessian Matrix:\n                 mu           ar1         omega        alpha1\nmu     -2749495.418 -24170.124984  4.546826e+06 -1.586692e+03\nar1      -24170.125   -390.266822  1.253879e+04 -6.733789e+00\nomega   4546825.784  12538.791045 -1.590043e+10 -7.069342e+05\nalpha1    -1586.692     -6.733789 -7.069342e+05 -1.425395e+02\nattr(,\"time\")\nTime difference of 0.004873753 secs\n\n--- END OF TRACE ---\n\n\nTime to Estimate Parameters:\n Time difference of 0.04393435 secs\n\nsummary(fit)\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(1, 0) + garch(1, 0), data = diff(log(gnp))) \n\nMean and Variance Equation:\n data ~ arma(1, 0) + garch(1, 0)\n&lt;environment: 0x55f169b16e90&gt;\n [data = diff(log(gnp))]\n\nConditional Distribution:\n norm \n\nCoefficient(s):\n        mu         ar1       omega      alpha1  \n0.00527795  0.36656257  0.00007331  0.19447133  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n        Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu     5.278e-03   8.996e-04    5.867 4.44e-09 ***\nar1    3.666e-01   7.514e-02    4.878 1.07e-06 ***\nomega  7.331e-05   9.011e-06    8.135 4.44e-16 ***\nalpha1 1.945e-01   9.554e-02    2.035   0.0418 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 722.2849    normalized:  3.253536 \n\nDescription:\n Mon Nov 18 16:04:59 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                 Statistic     p-Value\n Jarque-Bera Test   R    Chi^2   9.1180362 0.010472337\n Shapiro-Wilk Test  R    W       0.9842406 0.014336495\n Ljung-Box Test     R    Q(10)   9.8743260 0.451587525\n Ljung-Box Test     R    Q(15)  17.5585456 0.286584404\n Ljung-Box Test     R    Q(20)  23.4136291 0.268943681\n Ljung-Box Test     R^2  Q(10)  19.2821015 0.036822455\n Ljung-Box Test     R^2  Q(15)  33.2364834 0.004352735\n Ljung-Box Test     R^2  Q(20)  37.7425917 0.009518989\n LM Arch Test       R    TR^2   25.4162474 0.012969006\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-6.471035 -6.409726 -6.471669 -6.446282 \n\n\nThe estimates for the AR(1) component are ( = 0.005 ) (mu) and ( = 0.367 ) (ar1). The ARCH(1) parameters are ( _0 = 0 ) (omega) and ( _1 = 0.194 ), which is significant with a p-value around 0.02.\n\n\n3.1.1.3 Residual Diagnostics\nVarious tests were performed on the residuals and squared residuals. Notable results include:\n\nJarque-Bera Test: Chi-squared statistic of 9.118 (p-value = 0.010), suggesting some non-normal skewness and kurtosis in the residuals.\nShapiro-Wilk Test: W statistic of 0.984 (p-value = 0.014), indicating deviation from normality based on empirical order statistics.\nLjung-Box Tests:\n\nResiduals [R]: Q(20) statistic of 23.414 (p-value = 0.269), suggesting no significant autocorrelation in residuals.\nSquared Residuals [R^2]: Q(20) statistic of 37.743 (p-value = 0.010), indicating some dependence in squared residuals.\n\n\nThe diagnostics imply that the ARCH(1) model captures the non-constant variance pattern, with significant ARCH effects in the U.S. GNP series.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Capítulo 5</span>"
    ]
  },
  {
    "objectID": "cap5.html#arch-analysis-of-the-djia-returns",
    "href": "cap5.html#arch-analysis-of-the-djia-returns",
    "title": "3  Capítulo 5",
    "section": "3.2 ARCH Analysis of the DJIA Returns",
    "text": "3.2 ARCH Analysis of the DJIA Returns\n\n3.2.1 Background\nThe daily returns of the Dow Jones Industrial Average (DJIA) exhibit classic GARCH characteristics, including volatility clustering. Additionally, there is a low level of autocorrelation in the series itself. To capture both of these features, we fit an AR(1)-GARCH(1, 1) model to the series, assuming t-distributed errors for robustness in handling heavy tails.\n\n3.2.1.1 Data Preparation and Initial Analysis\nThe DJIA returns are computed as the daily log differences of closing prices. The ACF of the returns and squared returns reveal some autocorrelation in the returns and significant autocorrelation in the squared returns, suggesting the presence of GARCH effects. The initial analysis and autocorrelation functions can be plotted with:\n\nlibrary(xts)\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nlibrary(astsa)\n# Calculate daily returns\ndjiar &lt;- diff(log(djia$Close))[-1]\n\n# Autocorrelation analysis\nacf2(djiar)     # Shows slight autocorrelation in returns\n\n\n\n\n\n\n\n\n     [,1]  [,2] [,3]  [,4]  [,5] [,6]  [,7] [,8]  [,9] [,10] [,11] [,12] [,13]\nACF  -0.1 -0.06 0.05 -0.02 -0.06 0.01 -0.02 0.02 -0.01  0.04 -0.01  0.04  0.01\nPACF -0.1 -0.07 0.04 -0.02 -0.06 0.00 -0.02 0.03 -0.01  0.04 -0.01  0.04  0.02\n     [,14] [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25]\nACF  -0.04 -0.06  0.06  0.00 -0.07  0.02  0.05 -0.06  0.04  0.01 -0.01     0\nPACF -0.04 -0.06  0.04  0.02 -0.06  0.00  0.04 -0.04  0.03  0.00  0.00     0\n     [,26] [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37]\nACF      0  0.03 -0.03  0.01  0.02 -0.02  0.01  0.01 -0.09  0.03  0.03 -0.02\nPACF     0  0.04 -0.03  0.00  0.02  0.00  0.00  0.01 -0.07  0.01  0.02  0.01\n     [,38] [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49]\nACF      0  0.03  0.02  0.01  0.00 -0.06     0     0 -0.01  0.02  0.00 -0.04\nPACF     0  0.01  0.04  0.02  0.01 -0.06     0     0 -0.01  0.01 -0.01 -0.05\n     [,50] [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61]\nACF  -0.04 -0.02  0.03  0.00  0.00  0.01 -0.03  0.02  0.02 -0.07  0.02  0.02\nPACF -0.04 -0.03  0.01  0.01  0.02  0.01 -0.03  0.01  0.02 -0.05  0.01  0.02\n\nacf2(djiar^2)   # Shows strong autocorrelation in squared returns\n\n\n\n\n\n\n\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] [,11] [,12] [,13] [,14]\nACF   0.2 0.41 0.19 0.31 0.34 0.31 0.32 0.22 0.32  0.24  0.43  0.28  0.25  0.13\nPACF  0.2 0.39 0.08 0.15 0.25 0.13 0.11 0.01 0.11  0.05  0.23  0.08 -0.07 -0.16\n     [,15] [,16] [,17] [,18] [,19] [,20] [,21] [,22] [,23] [,24] [,25] [,26]\nACF   0.22  0.26  0.27  0.27  0.17  0.23  0.25  0.19  0.29  0.15  0.18  0.15\nPACF -0.03  0.04  0.03  0.05 -0.01  0.00  0.09 -0.09  0.07 -0.01  0.00  0.00\n     [,27] [,28] [,29] [,30] [,31] [,32] [,33] [,34] [,35] [,36] [,37] [,38]\nACF   0.28  0.22  0.22  0.14  0.16  0.20  0.14  0.25  0.10  0.17  0.12  0.16\nPACF  0.12  0.02 -0.03 -0.06  0.00  0.01 -0.06  0.06 -0.03 -0.05  0.02 -0.06\n     [,39] [,40] [,41] [,42] [,43] [,44] [,45] [,46] [,47] [,48] [,49] [,50]\nACF   0.15  0.10  0.08  0.08  0.11  0.13  0.13  0.08  0.09  0.11  0.07  0.08\nPACF -0.06 -0.06 -0.03  0.01  0.00  0.03  0.02 -0.01  0.00  0.06 -0.02 -0.05\n     [,51] [,52] [,53] [,54] [,55] [,56] [,57] [,58] [,59] [,60] [,61]\nACF   0.07  0.08  0.06  0.09  0.06  0.12  0.09  0.06  0.08  0.04  0.08\nPACF  0.03  0.04  0.02 -0.02 -0.09  0.08  0.06  0.00  0.01 -0.02  0.00\n\n\n\n\n3.2.1.2 Fitting an AR(1)-GARCH(1,1) Model\nThe fGarch package in R allows for fitting an AR(1)-GARCH(1, 1) model with t-distributed errors to account for potential heavy tails in the DJIA returns. The following code fits the model and provides a summary of the estimates:\n\nlibrary(fGarch)\n# Fit AR(1)-GARCH(1,1) model with t-distributed errors\ndjia.g &lt;- garchFit(~ arma(1, 0) + garch(1, 1), data = djiar, cond.dist = \"std\")\n\n\nSeries Initialization:\n ARMA Model:                arma\n Formula Mean:              ~ arma(1, 0)\n GARCH Model:               garch\n Formula Variance:          ~ garch(1, 1)\n ARMA Order:                1 0\n Max ARMA Order:            1\n GARCH Order:               1 1\n Max GARCH Order:           1\n Maximum Order:             1\n Conditional Dist:          std\n h.start:                   2\n llh.start:                 1\n Length of Series:          2517\n Recursion Init:            mci\n Series Scale:              0.01210097\n\nParameter Initialization:\n Initial Parameters:          $params\n Limits of Transformations:   $U, $V\n Which Parameters are Fixed?  $includes\n Parameter Matrix:\n                     U           V      params includes\n    mu     -0.15336279   0.1533628  0.01533395     TRUE\n    ar1    -0.99999999   1.0000000 -0.10129752     TRUE\n    omega   0.00000100 100.0000000  0.10000000     TRUE\n    alpha1  0.00000001   1.0000000  0.10000000     TRUE\n    gamma1 -0.99999999   1.0000000  0.10000000    FALSE\n    beta1   0.00000001   1.0000000  0.80000000     TRUE\n    delta   0.00000000   2.0000000  2.00000000    FALSE\n    skew    0.10000000  10.0000000  1.00000000    FALSE\n    shape   1.00000000  10.0000000  4.00000000     TRUE\n Index List of Parameters to be Optimized:\n    mu    ar1  omega alpha1  beta1  shape \n     1      2      3      4      6      9 \n Persistence:                  0.9 \n\n\n--- START OF TRACE ---\nSelected Algorithm: nlminb \n\nR coded nlminb Solver: \n\n  0:     2966.5649: 0.0153339 -0.101298 0.100000 0.100000 0.800000  4.00000\n  1:     2944.7772: 0.0153351 -0.0991717 0.0803850 0.105688 0.794226  3.99986\n  2:     2910.9438: 0.0153425 -0.0864389 0.0198837 0.162520 0.809278  4.00053\n  3:     2891.9985: 0.0153434 -0.0855696 0.0349997 0.168391 0.817707  4.00077\n  4:     2882.6364: 0.0153798 -0.0489475 0.0211398 0.164588 0.840541  4.00286\n  5:     2881.9301: 0.0153823 -0.0484838 0.0204238 0.166791 0.845394  4.00314\n  6:     2881.5679: 0.0153882 -0.0491671 0.0164038 0.164251 0.847793  4.00365\n  7:     2880.9558: 0.0153959 -0.0493820 0.0174436 0.161528 0.852261  4.00434\n  8:     2880.6291: 0.0154049 -0.0492037 0.0154767 0.158055 0.855764  4.00516\n  9:     2880.3565: 0.0154166 -0.0487266 0.0159444 0.154678 0.859712  4.00626\n 10:     2880.1243: 0.0154339 -0.0478609 0.0146355 0.150737 0.862389  4.00794\n 11:     2879.9530: 0.0154638 -0.0476576 0.0149075 0.147959 0.865388  4.01087\n 12:     2879.8201: 0.0155048 -0.0483157 0.0138749 0.146453 0.866639  4.01497\n 13:     2879.7180: 0.0155488 -0.0477882 0.0141937 0.145774 0.867564  4.01936\n 14:     2879.4141: 0.0157808 -0.0423403 0.0141381 0.141896 0.868277  4.04243\n 15:     2878.3878: 0.0174980 -0.0569546 0.00848860 0.140360 0.874674  4.21079\n 16:     2878.2927: 0.0193492 -0.0502757 0.0142265 0.160178 0.862535  4.37200\n 17:     2876.3368: 0.0203874 -0.0461078 0.0117835 0.167862 0.852926  4.44641\n 18:     2874.7342: 0.0206459 -0.0555791 0.0130825 0.138689 0.869340  4.42804\n 19:     2874.6724: 0.0206462 -0.0554532 0.0123098 0.138461 0.869109  4.42806\n 20:     2874.6548: 0.0206553 -0.0553211 0.0125311 0.138489 0.869292  4.42857\n 21:     2874.6394: 0.0206635 -0.0550129 0.0122399 0.138334 0.869389  4.42904\n 22:     2874.6234: 0.0206727 -0.0548830 0.0124501 0.138349 0.869554  4.42956\n 23:     2874.6091: 0.0206813 -0.0546059 0.0121936 0.138189 0.869619  4.43005\n 24:     2874.5943: 0.0206905 -0.0544775 0.0123915 0.138195 0.869769  4.43057\n 25:     2874.5807: 0.0206992 -0.0542217 0.0121569 0.138035 0.869816  4.43107\n 26:     2874.5667: 0.0207085 -0.0540958 0.0123467 0.138034 0.869956  4.43160\n 27:     2874.5537: 0.0207174 -0.0538569 0.0121278 0.137877 0.869991  4.43211\n 28:     2874.5403: 0.0207267 -0.0537340 0.0123108 0.137872 0.870123  4.43264\n 29:     2874.5278: 0.0207356 -0.0535096 0.0121041 0.137717 0.870149  4.43315\n 30:     2874.5149: 0.0207450 -0.0533899 0.0122810 0.137709 0.870275  4.43368\n 31:     2874.5028: 0.0207540 -0.0531784 0.0120844 0.137558 0.870296  4.43419\n 32:     2874.4904: 0.0207635 -0.0530620 0.0122555 0.137548 0.870416  4.43473\n 33:     2874.4787: 0.0207726 -0.0528620 0.0120673 0.137401 0.870433  4.43525\n 34:     2874.4667: 0.0207820 -0.0527489 0.0122328 0.137390 0.870549  4.43578\n 35:     2874.4553: 0.0207912 -0.0525594 0.0120521 0.137247 0.870562  4.43630\n 36:     2874.4437: 0.0208007 -0.0524496 0.0122124 0.137235 0.870674  4.43684\n 37:     2874.4326: 0.0208099 -0.0522698 0.0120383 0.137096 0.870686  4.43736\n 38:     2874.4213: 0.0208194 -0.0521631 0.0121935 0.137083 0.870794  4.43789\n 39:     2874.4105: 0.0208287 -0.0519922 0.0120255 0.136949 0.870804  4.43842\n 40:     2874.3995: 0.0208383 -0.0518887 0.0121759 0.136934 0.870909  4.43895\n 41:     2874.3889: 0.0208476 -0.0517261 0.0120134 0.136805 0.870917  4.43948\n 42:     2874.3782: 0.0208572 -0.0516257 0.0121593 0.136790 0.871020  4.44002\n 43:     2874.3679: 0.0208666 -0.0514707 0.0120019 0.136664 0.871027  4.44055\n 44:     2874.3574: 0.0208762 -0.0513733 0.0121435 0.136649 0.871126  4.44108\n 45:     2874.3474: 0.0208856 -0.0512255 0.0119909 0.136527 0.871132  4.44161\n 46:     2874.3371: 0.0208952 -0.0511310 0.0121283 0.136511 0.871229  4.44215\n 47:     2874.3273: 0.0209047 -0.0509898 0.0119802 0.136393 0.871235  4.44268\n 48:     2874.3172: 0.0209144 -0.0508982 0.0121136 0.136378 0.871329  4.44321\n 49:     2874.3076: 0.0209239 -0.0507633 0.0119698 0.136263 0.871333  4.44374\n 50:     2874.2977: 0.0209336 -0.0506745 0.0120995 0.136247 0.871425  4.44428\n 51:     2874.2883: 0.0209431 -0.0505454 0.0119597 0.136136 0.871429  4.44481\n 52:     2874.2786: 0.0209528 -0.0504593 0.0120858 0.136120 0.871519  4.44535\n 53:     2874.2693: 0.0209624 -0.0503357 0.0119499 0.136012 0.871522  4.44588\n 54:     2874.2598: 0.0209721 -0.0502524 0.0120725 0.135996 0.871609  4.44641\n 55:     2874.2507: 0.0209817 -0.0501339 0.0119403 0.135891 0.871612  4.44694\n 56:     2874.2414: 0.0209915 -0.0500532 0.0120596 0.135875 0.871697  4.44748\n 57:     2874.2324: 0.0210011 -0.0499396 0.0119308 0.135773 0.871700  4.44801\n 58:     2867.2502: 0.0374784 -0.0465701 0.0140411 0.146417 0.849465  5.34102\n 59:     2865.2784: 0.0416151 -0.0568642 0.0115908 0.133741 0.863377  5.47854\n 60:     2864.2803: 0.0508382 -0.0551564 0.0133502 0.137240 0.863921  5.39588\n 61:     2861.7759: 0.0711833 -0.0514599 0.0116358 0.126101 0.868871  5.59207\n 62:     2861.6890: 0.0720867 -0.0568481 0.0103059 0.127155 0.870034  5.77185\n 63:     2861.6323: 0.0712594 -0.0580540 0.0105727 0.123206 0.871658  5.89046\n 64:     2861.6059: 0.0710100 -0.0558795 0.0112294 0.125022 0.869481  5.92682\n 65:     2861.6004: 0.0709184 -0.0550834 0.0110167 0.124375 0.870072  5.96212\n 66:     2861.6000: 0.0709561 -0.0553609 0.0109915 0.124443 0.870018  5.97670\n 67:     2861.6000: 0.0709474 -0.0553160 0.0109949 0.124443 0.870016  5.97866\n 68:     2861.6000: 0.0709478 -0.0553153 0.0109957 0.124445 0.870013  5.97877\n\nFinal Estimate of the Negative LLH:\n LLH:  -8249.619    norm LLH:  -3.27756 \n           mu           ar1         omega        alpha1         beta1 \n 8.585376e-04 -5.531526e-02  1.610146e-06  1.244449e-01  8.700127e-01 \n        shape \n 5.978770e+00 \n\nR-optimhess Difference Approximated Hessian Matrix:\n                  mu           ar1         omega        alpha1         beta1\nmu     -4.791651e+07 -4.661861e+04 -1.205634e+09 -3.466862e+04 -9.149105e+04\nar1    -4.661861e+04 -2.490882e+03 -1.234074e+06 -8.203053e+00 -9.744031e+01\nomega  -1.205634e+09 -1.234074e+06 -1.703962e+13 -5.515881e+08 -8.452220e+08\nalpha1 -3.466862e+04 -8.203053e+00 -5.515881e+08 -3.611224e+04 -4.469020e+04\nbeta1  -9.149105e+04 -9.744031e+01 -8.452220e+08 -4.469020e+04 -6.270062e+04\nshape  -9.970351e+02  8.557532e-02 -3.050363e+06 -1.832755e+02 -2.340762e+02\n               shape\nmu     -9.970351e+02\nar1     8.557532e-02\nomega  -3.050363e+06\nalpha1 -1.832755e+02\nbeta1  -2.340762e+02\nshape  -2.547431e+00\nattr(,\"time\")\nTime difference of 0.0685904 secs\n\n--- END OF TRACE ---\n\n\nTime to Estimate Parameters:\n Time difference of 0.3414211 secs\n\nsummary(djia.g)\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(1, 0) + garch(1, 1), data = djiar, cond.dist = \"std\") \n\nMean and Variance Equation:\n data ~ arma(1, 0) + garch(1, 1)\n&lt;environment: 0x55f168599750&gt;\n [data = djiar]\n\nConditional Distribution:\n std \n\nCoefficient(s):\n         mu          ar1        omega       alpha1        beta1        shape  \n 8.5854e-04  -5.5315e-02   1.6101e-06   1.2444e-01   8.7001e-01   5.9788e+00  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu      8.585e-04   1.470e-04    5.842 5.16e-09 ***\nar1    -5.532e-02   2.023e-02   -2.735 0.006238 ** \nomega   1.610e-06   4.459e-07    3.611 0.000305 ***\nalpha1  1.244e-01   1.660e-02    7.496 6.55e-14 ***\nbeta1   8.700e-01   1.526e-02   57.022  &lt; 2e-16 ***\nshape   5.979e+00   7.917e-01    7.552 4.31e-14 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 8249.619    normalized:  3.27756 \n\nDescription:\n Mon Nov 18 16:05:00 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                  Statistic    p-Value\n Jarque-Bera Test   R    Chi^2  310.0081692 0.00000000\n Shapiro-Wilk Test  R    W        0.9820293 0.00000000\n Ljung-Box Test     R    Q(10)   16.8224601 0.07838596\n Ljung-Box Test     R    Q(15)   26.4481303 0.03356806\n Ljung-Box Test     R    Q(20)   28.7109935 0.09360790\n Ljung-Box Test     R^2  Q(10)   15.3676143 0.11922299\n Ljung-Box Test     R^2  Q(15)   19.1365044 0.20761469\n Ljung-Box Test     R^2  Q(20)   22.9288237 0.29230296\n LM Arch Test       R    TR^2    15.0397685 0.23926882\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-6.550353 -6.536453 -6.550364 -6.545309 \n\n#plot(djia.g)  # View model diagnostic plots\n\n\n\n3.2.1.3 Summary of Results\nThe AR(1) coefficient ((_1)) is -0.05531, indicating a slight negative autocorrelation in returns. The GARCH parameters (_1 = 0.1244) and (_1 = 0.8700) are both significant, suggesting the model captures the observed volatility clustering in DJIA returns effectively.\n\n\n3.2.1.4 Residual Diagnostics\nResidual diagnostics are as follows:\n\nLjung-Box Test:\n\nResiduals ([R]): (Q(10) = 16.82) (p-value = 0.0786), suggesting no significant autocorrelation in residuals.\nSquared Residuals ([R^2]): (Q(10) = 15.39) (p-value = 0.1184), indicating no remaining ARCH effects.\n\n\nThese tests confirm that the AR(1)-GARCH(1, 1) model adequately captures the conditional heteroskedasticity in the DJIA returns.\n\n\n3.2.1.5 GARCH Predictions of Volatility\nWe explored GARCH predictions of volatility, particularly around the financial crisis of 2008. One-step-ahead predictions of volatility ((^2)) were plotted along with the observed data, as shown in Figure 5.6. This provides insight into how volatility predictions varied during a period of high market uncertainty.\n\n# Assuming `djia` is already loaded with a 'Close' column for DJIA closing prices\n# Calculate daily returns of DJIA\ndjiar &lt;- diff(log(djia$Close))[-1]\n\n# Fit the AR(1)-GARCH(1,1) model with t-distributed errors\ndjia.g &lt;- garchFit(~ arma(1, 0) + garch(1, 1), data = djiar, cond.dist = \"std\")\n\n\nSeries Initialization:\n ARMA Model:                arma\n Formula Mean:              ~ arma(1, 0)\n GARCH Model:               garch\n Formula Variance:          ~ garch(1, 1)\n ARMA Order:                1 0\n Max ARMA Order:            1\n GARCH Order:               1 1\n Max GARCH Order:           1\n Maximum Order:             1\n Conditional Dist:          std\n h.start:                   2\n llh.start:                 1\n Length of Series:          2517\n Recursion Init:            mci\n Series Scale:              0.01210097\n\nParameter Initialization:\n Initial Parameters:          $params\n Limits of Transformations:   $U, $V\n Which Parameters are Fixed?  $includes\n Parameter Matrix:\n                     U           V      params includes\n    mu     -0.15336279   0.1533628  0.01533395     TRUE\n    ar1    -0.99999999   1.0000000 -0.10129752     TRUE\n    omega   0.00000100 100.0000000  0.10000000     TRUE\n    alpha1  0.00000001   1.0000000  0.10000000     TRUE\n    gamma1 -0.99999999   1.0000000  0.10000000    FALSE\n    beta1   0.00000001   1.0000000  0.80000000     TRUE\n    delta   0.00000000   2.0000000  2.00000000    FALSE\n    skew    0.10000000  10.0000000  1.00000000    FALSE\n    shape   1.00000000  10.0000000  4.00000000     TRUE\n Index List of Parameters to be Optimized:\n    mu    ar1  omega alpha1  beta1  shape \n     1      2      3      4      6      9 \n Persistence:                  0.9 \n\n\n--- START OF TRACE ---\nSelected Algorithm: nlminb \n\nR coded nlminb Solver: \n\n  0:     2966.5649: 0.0153339 -0.101298 0.100000 0.100000 0.800000  4.00000\n  1:     2944.7772: 0.0153351 -0.0991717 0.0803850 0.105688 0.794226  3.99986\n  2:     2910.9438: 0.0153425 -0.0864389 0.0198837 0.162520 0.809278  4.00053\n  3:     2891.9985: 0.0153434 -0.0855696 0.0349997 0.168391 0.817707  4.00077\n  4:     2882.6364: 0.0153798 -0.0489475 0.0211398 0.164588 0.840541  4.00286\n  5:     2881.9301: 0.0153823 -0.0484838 0.0204238 0.166791 0.845394  4.00314\n  6:     2881.5679: 0.0153882 -0.0491671 0.0164038 0.164251 0.847793  4.00365\n  7:     2880.9558: 0.0153959 -0.0493820 0.0174436 0.161528 0.852261  4.00434\n  8:     2880.6291: 0.0154049 -0.0492037 0.0154767 0.158055 0.855764  4.00516\n  9:     2880.3565: 0.0154166 -0.0487266 0.0159444 0.154678 0.859712  4.00626\n 10:     2880.1243: 0.0154339 -0.0478609 0.0146355 0.150737 0.862389  4.00794\n 11:     2879.9530: 0.0154638 -0.0476576 0.0149075 0.147959 0.865388  4.01087\n 12:     2879.8201: 0.0155048 -0.0483157 0.0138749 0.146453 0.866639  4.01497\n 13:     2879.7180: 0.0155488 -0.0477882 0.0141937 0.145774 0.867564  4.01936\n 14:     2879.4141: 0.0157808 -0.0423403 0.0141381 0.141896 0.868277  4.04243\n 15:     2878.3878: 0.0174980 -0.0569546 0.00848860 0.140360 0.874674  4.21079\n 16:     2878.2927: 0.0193492 -0.0502757 0.0142265 0.160178 0.862535  4.37200\n 17:     2876.3368: 0.0203874 -0.0461078 0.0117835 0.167862 0.852926  4.44641\n 18:     2874.7342: 0.0206459 -0.0555791 0.0130825 0.138689 0.869340  4.42804\n 19:     2874.6724: 0.0206462 -0.0554532 0.0123098 0.138461 0.869109  4.42806\n 20:     2874.6548: 0.0206553 -0.0553211 0.0125311 0.138489 0.869292  4.42857\n 21:     2874.6394: 0.0206635 -0.0550129 0.0122399 0.138334 0.869389  4.42904\n 22:     2874.6234: 0.0206727 -0.0548830 0.0124501 0.138349 0.869554  4.42956\n 23:     2874.6091: 0.0206813 -0.0546059 0.0121936 0.138189 0.869619  4.43005\n 24:     2874.5943: 0.0206905 -0.0544775 0.0123915 0.138195 0.869769  4.43057\n 25:     2874.5807: 0.0206992 -0.0542217 0.0121569 0.138035 0.869816  4.43107\n 26:     2874.5667: 0.0207085 -0.0540958 0.0123467 0.138034 0.869956  4.43160\n 27:     2874.5537: 0.0207174 -0.0538569 0.0121278 0.137877 0.869991  4.43211\n 28:     2874.5403: 0.0207267 -0.0537340 0.0123108 0.137872 0.870123  4.43264\n 29:     2874.5278: 0.0207356 -0.0535096 0.0121041 0.137717 0.870149  4.43315\n 30:     2874.5149: 0.0207450 -0.0533899 0.0122810 0.137709 0.870275  4.43368\n 31:     2874.5028: 0.0207540 -0.0531784 0.0120844 0.137558 0.870296  4.43419\n 32:     2874.4904: 0.0207635 -0.0530620 0.0122555 0.137548 0.870416  4.43473\n 33:     2874.4787: 0.0207726 -0.0528620 0.0120673 0.137401 0.870433  4.43525\n 34:     2874.4667: 0.0207820 -0.0527489 0.0122328 0.137390 0.870549  4.43578\n 35:     2874.4553: 0.0207912 -0.0525594 0.0120521 0.137247 0.870562  4.43630\n 36:     2874.4437: 0.0208007 -0.0524496 0.0122124 0.137235 0.870674  4.43684\n 37:     2874.4326: 0.0208099 -0.0522698 0.0120383 0.137096 0.870686  4.43736\n 38:     2874.4213: 0.0208194 -0.0521631 0.0121935 0.137083 0.870794  4.43789\n 39:     2874.4105: 0.0208287 -0.0519922 0.0120255 0.136949 0.870804  4.43842\n 40:     2874.3995: 0.0208383 -0.0518887 0.0121759 0.136934 0.870909  4.43895\n 41:     2874.3889: 0.0208476 -0.0517261 0.0120134 0.136805 0.870917  4.43948\n 42:     2874.3782: 0.0208572 -0.0516257 0.0121593 0.136790 0.871020  4.44002\n 43:     2874.3679: 0.0208666 -0.0514707 0.0120019 0.136664 0.871027  4.44055\n 44:     2874.3574: 0.0208762 -0.0513733 0.0121435 0.136649 0.871126  4.44108\n 45:     2874.3474: 0.0208856 -0.0512255 0.0119909 0.136527 0.871132  4.44161\n 46:     2874.3371: 0.0208952 -0.0511310 0.0121283 0.136511 0.871229  4.44215\n 47:     2874.3273: 0.0209047 -0.0509898 0.0119802 0.136393 0.871235  4.44268\n 48:     2874.3172: 0.0209144 -0.0508982 0.0121136 0.136378 0.871329  4.44321\n 49:     2874.3076: 0.0209239 -0.0507633 0.0119698 0.136263 0.871333  4.44374\n 50:     2874.2977: 0.0209336 -0.0506745 0.0120995 0.136247 0.871425  4.44428\n 51:     2874.2883: 0.0209431 -0.0505454 0.0119597 0.136136 0.871429  4.44481\n 52:     2874.2786: 0.0209528 -0.0504593 0.0120858 0.136120 0.871519  4.44535\n 53:     2874.2693: 0.0209624 -0.0503357 0.0119499 0.136012 0.871522  4.44588\n 54:     2874.2598: 0.0209721 -0.0502524 0.0120725 0.135996 0.871609  4.44641\n 55:     2874.2507: 0.0209817 -0.0501339 0.0119403 0.135891 0.871612  4.44694\n 56:     2874.2414: 0.0209915 -0.0500532 0.0120596 0.135875 0.871697  4.44748\n 57:     2874.2324: 0.0210011 -0.0499396 0.0119308 0.135773 0.871700  4.44801\n 58:     2867.2502: 0.0374784 -0.0465701 0.0140411 0.146417 0.849465  5.34102\n 59:     2865.2784: 0.0416151 -0.0568642 0.0115908 0.133741 0.863377  5.47854\n 60:     2864.2803: 0.0508382 -0.0551564 0.0133502 0.137240 0.863921  5.39588\n 61:     2861.7759: 0.0711833 -0.0514599 0.0116358 0.126101 0.868871  5.59207\n 62:     2861.6890: 0.0720867 -0.0568481 0.0103059 0.127155 0.870034  5.77185\n 63:     2861.6323: 0.0712594 -0.0580540 0.0105727 0.123206 0.871658  5.89046\n 64:     2861.6059: 0.0710100 -0.0558795 0.0112294 0.125022 0.869481  5.92682\n 65:     2861.6004: 0.0709184 -0.0550834 0.0110167 0.124375 0.870072  5.96212\n 66:     2861.6000: 0.0709561 -0.0553609 0.0109915 0.124443 0.870018  5.97670\n 67:     2861.6000: 0.0709474 -0.0553160 0.0109949 0.124443 0.870016  5.97866\n 68:     2861.6000: 0.0709478 -0.0553153 0.0109957 0.124445 0.870013  5.97877\n\nFinal Estimate of the Negative LLH:\n LLH:  -8249.619    norm LLH:  -3.27756 \n           mu           ar1         omega        alpha1         beta1 \n 8.585376e-04 -5.531526e-02  1.610146e-06  1.244449e-01  8.700127e-01 \n        shape \n 5.978770e+00 \n\nR-optimhess Difference Approximated Hessian Matrix:\n                  mu           ar1         omega        alpha1         beta1\nmu     -4.791651e+07 -4.661861e+04 -1.205634e+09 -3.466862e+04 -9.149105e+04\nar1    -4.661861e+04 -2.490882e+03 -1.234074e+06 -8.203053e+00 -9.744031e+01\nomega  -1.205634e+09 -1.234074e+06 -1.703962e+13 -5.515881e+08 -8.452220e+08\nalpha1 -3.466862e+04 -8.203053e+00 -5.515881e+08 -3.611224e+04 -4.469020e+04\nbeta1  -9.149105e+04 -9.744031e+01 -8.452220e+08 -4.469020e+04 -6.270062e+04\nshape  -9.970351e+02  8.557532e-02 -3.050363e+06 -1.832755e+02 -2.340762e+02\n               shape\nmu     -9.970351e+02\nar1     8.557532e-02\nomega  -3.050363e+06\nalpha1 -1.832755e+02\nbeta1  -2.340762e+02\nshape  -2.547431e+00\nattr(,\"time\")\nTime difference of 0.06105161 secs\n\n--- END OF TRACE ---\n\n\nTime to Estimate Parameters:\n Time difference of 0.3197961 secs\n\n# Extract fitted volatility (sigma) from the GARCH model\npredicted_volatility &lt;- djia.g@sigma.t\n\n# Set up a time series index (assuming `djiar` has time index matching `predicted_volatility`)\ndates &lt;- index(djiar)\n\n# Plot the observed returns and predicted volatility\nplot(dates, djiar, type = \"l\", col = \"blue\", ylab = \"Returns and Volatility\", xlab = \"Date\", main = \"DJIA Returns and GARCH(1,1) Predicted Volatility\")\nlines(dates, predicted_volatility, col = \"red\", lty = 2)\n\n# Add a legend\nlegend(\"topright\", legend = c(\"Observed Returns\", \"Predicted Volatility (GARCH)\"), col = c(\"blue\", \"red\"), lty = c(1, 2))",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Capítulo 5</span>"
    ]
  },
  {
    "objectID": "cap5.html#aparch-analysis-of-the-djia-returns",
    "href": "cap5.html#aparch-analysis-of-the-djia-returns",
    "title": "3  Capítulo 5",
    "section": "3.3 APARCH Analysis of the DJIA Returns",
    "text": "3.3 APARCH Analysis of the DJIA Returns\n\n3.3.1 Background\nIn this example, we apply an AR-APARCH model to the DJIA returns, as discussed in Example 5.5. Similar to the previous example, an AR(1) term is included in the model to account for the conditional mean. We assume that the error process follows an APARCH(1,1) structure with t-distributed errors to capture asymmetry and heavy tails in the return series. The model equation is:\n[ r_t = _t + y_t ]\nwhere ( _t ) is an AR(1) component, and ( y_t ) follows an APARCH process with conditional variance modeled as specified in Equation (5.53).\n\n3.3.1.1 Model Fitting and Summary\nWe use the fGarch package in R to fit the AR-APARCH(1,1) model to the DJIA returns. The following code performs the fitting and provides a summary of the estimated parameters:\n\nlibrary(xts)\nlibrary(fGarch)\n\n# Calculate daily returns of DJIA\ndjiar &lt;- diff(log(djia$Close))[-1]\n\n# Fit AR(1)-APARCH(1,1) model with t-distributed errors\ndjia.ap &lt;- garchFit(~ arma(1, 0) + aparch(1, 1), data = djiar, cond.dist = \"std\")\n\n\nSeries Initialization:\n ARMA Model:                arma\n Formula Mean:              ~ arma(1, 0)\n GARCH Model:               aparch\n Formula Variance:          ~ aparch(1, 1)\n ARMA Order:                1 0\n Max ARMA Order:            1\n GARCH Order:               1 1\n Max GARCH Order:           1\n Maximum Order:             1\n Conditional Dist:          std\n h.start:                   2\n llh.start:                 1\n Length of Series:          2517\n Recursion Init:            mci\n Series Scale:              0.01210097\n\nParameter Initialization:\n Initial Parameters:          $params\n Limits of Transformations:   $U, $V\n Which Parameters are Fixed?  $includes\n Parameter Matrix:\n                     U           V      params includes\n    mu     -0.15336279   0.1533628  0.01533395     TRUE\n    ar1    -0.99999999   1.0000000 -0.10129752     TRUE\n    omega   0.00000100 100.0000000  0.10000000     TRUE\n    alpha1  0.00000001   1.0000000  0.10000000     TRUE\n    gamma1 -0.99999999   1.0000000  0.10000000     TRUE\n    beta1   0.00000001   1.0000000  0.80000000     TRUE\n    delta   0.00000000   2.0000000  2.00000000     TRUE\n    skew    0.10000000  10.0000000  1.00000000    FALSE\n    shape   1.00000000  10.0000000  4.00000000     TRUE\n Index List of Parameters to be Optimized:\n    mu    ar1  omega alpha1 gamma1  beta1  delta  shape \n     1      2      3      4      5      6      7      9 \n Persistence:                  0.901 \n\n\n--- START OF TRACE ---\nSelected Algorithm: nlminb \n\nR coded nlminb Solver: \n\n  0:     2956.2216: 0.0153339 -0.101298 0.100000 0.100000 0.100000 0.800000  2.00000  4.00000\n  1:     2933.7103: 0.0153349 -0.0994952 0.0816702 0.105086 0.101441 0.794372  1.99989  3.99987\n  2:     2915.7667: 0.0153364 -0.0969033 0.0659019 0.116672 0.103734 0.795785  2.00000  3.99997\n  3:     2868.4755: 0.0153445 -0.0848310 0.0240071 0.174769 0.115713 0.826263  2.00000  4.00127\n  4:     2866.0067: 0.0153484 -0.0806140 0.0223751 0.179103 0.121376 0.835430  2.00000  4.00180\n  5:     2865.7487: 0.0153525 -0.0768014 0.0131222 0.176106 0.127212 0.838921  1.99992  4.00228\n  6:     2863.1420: 0.0153538 -0.0756613 0.0180358 0.176968 0.129016 0.841888  2.00000  4.00245\n  7:     2856.2449: 0.0153875 -0.0476167 0.00965957 0.170079 0.175758 0.860641  1.99995  4.00607\n  8:     2853.4108: 0.0154249 -0.0295659 0.0252436 0.165440 0.224654 0.839180  1.99938  4.00936\n  9:     2853.0448: 0.0154257 -0.0299884 0.0155240 0.163926 0.225784 0.836602  1.99923  4.00941\n 10:     2850.4568: 0.0154262 -0.0301237 0.0197886 0.165257 0.226344 0.839033  1.99934  4.00951\n 11:     2849.7888: 0.0154284 -0.0309518 0.0166941 0.166172 0.229095 0.841761  1.99938  4.00981\n 12:     2849.3808: 0.0154318 -0.0322155 0.0188338 0.167018 0.233146 0.843406  1.99940  4.01023\n 13:     2848.6297: 0.0154356 -0.0334350 0.0170783 0.166875 0.237702 0.842613  1.99931  4.01065\n 14:     2848.2726: 0.0154392 -0.0343915 0.0177679 0.167228 0.241904 0.845217  1.99934  4.01111\n 15:     2847.6118: 0.0154433 -0.0353050 0.0161095 0.167003 0.246509 0.844166  1.99923  4.01155\n 16:     2839.6580: 0.0155653 -0.0553493 0.0222431 0.178380 0.380319 0.824116  1.99673  4.02506\n 17:     2839.3512: 0.0155658 -0.0550968 0.0204751 0.177743 0.380646 0.823994  1.99667  4.02511\n 18:     2838.9545: 0.0155699 -0.0530330 0.0209096 0.175844 0.383429 0.828959  1.99653  4.02564\n 19:     2838.4453: 0.0155822 -0.0490947 0.0190161 0.175891 0.387313 0.827244  1.99538  4.02695\n 20:     2838.2004: 0.0155983 -0.0451962 0.0202650 0.176410 0.391417 0.827097  1.99373  4.02871\n 21:     2837.9655: 0.0156258 -0.0427583 0.0191101 0.176071 0.394484 0.826442  1.99035  4.03170\n 22:     2837.7164: 0.0156586 -0.0433600 0.0199725 0.175167 0.395086 0.827962  1.98601  4.03528\n 23:     2837.4877: 0.0156899 -0.0444502 0.0188067 0.173444 0.395339 0.829067  1.98177  4.03873\n 24:     2837.2542: 0.0157227 -0.0444858 0.0196845 0.172775 0.396371 0.830254  1.97741  4.04241\n 25:     2837.0243: 0.0157544 -0.0436348 0.0187730 0.171889 0.398204 0.830412  1.97322  4.04605\n 26:     2836.8012: 0.0157867 -0.0431291 0.0196235 0.171422 0.399701 0.831316  1.96895  4.04979\n 27:     2836.5848: 0.0158187 -0.0432528 0.0186828 0.170271 0.400698 0.831832  1.96461  4.05354\n 28:     2836.3739: 0.0158508 -0.0434055 0.0194501 0.169589 0.401625 0.832972  1.96028  4.05734\n 29:     2836.1649: 0.0158827 -0.0431931 0.0185802 0.168628 0.402869 0.833293  1.95598  4.06117\n 30:     2835.9597: 0.0159145 -0.0428368 0.0193553 0.168170 0.404207 0.834156  1.95172  4.06504\n 31:     2835.7585: 0.0159462 -0.0426654 0.0185188 0.167252 0.405404 0.834462  1.94744  4.06892\n 32:     2835.5621: 0.0159779 -0.0426467 0.0192452 0.166709 0.406444 0.835427  1.94316  4.07284\n 33:     2835.3683: 0.0160095 -0.0425704 0.0184359 0.165808 0.407541 0.835744  1.93888  4.07678\n 34:     2835.1779: 0.0160411 -0.0423779 0.0191523 0.165365 0.408710 0.836588  1.93464  4.08076\n 35:     2834.9901: 0.0160726 -0.0421938 0.0183829 0.164555 0.409882 0.836832  1.93040  4.08474\n 36:     2834.8061: 0.0161042 -0.0420878 0.0190714 0.164114 0.410966 0.837674  1.92617  4.08876\n 37:     2834.6246: 0.0161357 -0.0420095 0.0183222 0.163312 0.412035 0.837939  1.92194  4.09279\n 38:     2834.4462: 0.0161673 -0.0418854 0.0189920 0.162910 0.413121 0.838736  1.91773  4.09685\n 39:     2834.2698: 0.0161987 -0.0417390 0.0182750 0.162177 0.414233 0.838949  1.91352  4.10092\n 40:     2834.0965: 0.0162303 -0.0416151 0.0189257 0.161805 0.415308 0.839711  1.90934  4.10501\n 41:     2833.9253: 0.0162618 -0.0415237 0.0182290 0.161092 0.416362 0.839927  1.90514  4.10911\n 42:     2833.7569: 0.0162934 -0.0414219 0.0188604 0.160741 0.417408 0.840667  1.90097  4.11324\n 43:     2833.5901: 0.0163250 -0.0413048 0.0181889 0.160075 0.418472 0.840852  1.89680  4.11737\n 44:     2833.4261: 0.0163566 -0.0411882 0.0188048 0.159756 0.419520 0.841555  1.89265  4.12152\n 45:     2833.2636: 0.0163883 -0.0410937 0.0181531 0.159117 0.420555 0.841733  1.88849  4.12568\n 46:     2833.1037: 0.0164201 -0.0410006 0.0187518 0.158815 0.421575 0.842418  1.88435  4.12986\n 47:     2832.9452: 0.0164519 -0.0409017 0.0181207 0.158211 0.422604 0.842576  1.88021  4.13404\n 48:     2832.7890: 0.0164838 -0.0407973 0.0187056 0.157937 0.423625 0.843229  1.87609  4.13824\n 49:     2832.6341: 0.0165157 -0.0407067 0.0180929 0.157360 0.424638 0.843376  1.87196  4.14245\n 50:     2832.4815: 0.0165478 -0.0406213 0.0186628 0.157101 0.425636 0.844012  1.86786  4.14667\n 51:     2832.3300: 0.0165799 -0.0405346 0.0180677 0.156553 0.426639 0.844146  1.86374  4.15089\n 52:     2832.1806: 0.0166122 -0.0404422 0.0186253 0.156317 0.427635 0.844755  1.85965  4.15513\n 53:     2832.0323: 0.0166446 -0.0403594 0.0180468 0.155794 0.428627 0.844877  1.85556  4.15937\n 54:     2831.8860: 0.0166771 -0.0402797 0.0185914 0.155574 0.429607 0.845469  1.85148  4.16362\n 55:     2831.7406: 0.0167096 -0.0402025 0.0180284 0.155074 0.430588 0.845580  1.84740  4.16787\n 56:     2831.5971: 0.0167424 -0.0401200 0.0185619 0.154873 0.431564 0.846150  1.84334  4.17214\n 57:     2823.2275: 0.0241723 -0.0178143 0.0279421 0.154694 0.570032 0.852884 0.917653  5.13851\n 58:     2823.0077: 0.0243118 -0.0209096 0.0315367 0.155146 0.569922 0.857579 0.900545  5.15526\n 59:     2820.5079: 0.0243219 -0.0208801 0.0294576 0.153786 0.569402 0.856246 0.913322  5.15707\n 60:     2820.0209: 0.0243261 -0.0259732 0.0222170 0.153394 0.574190 0.864662 0.914968  5.15741\n 61:     2819.2340: 0.0243310 -0.0260678 0.0232473 0.153113 0.573853 0.865714 0.928086  5.15821\n 62:     2814.8421: 0.0250357 -0.0398787 0.0179842 0.132129 0.560768 0.879198  1.17994  5.25066\n 63:     2814.8331: 0.0254014 -0.0459558 0.0176973 0.127988 0.560005 0.881149  1.15004  5.28917\n 64:     2814.4735: 0.0257755 -0.0467011 0.0185883 0.127078 0.563492 0.882539  1.14177  5.29761\n 65:     2814.2272: 0.0257944 -0.0428649 0.0182816 0.128524 0.566955 0.880937  1.15745  5.27588\n 66:     2813.7698: 0.0261858 -0.0419618 0.0205550 0.126865 0.575104 0.878813  1.16041  5.27253\n 67:     2813.4645: 0.0265757 -0.0420537 0.0199896 0.126314 0.578671 0.878986  1.15776  5.28170\n 68:     2812.8046: 0.0272480 -0.0320793 0.0193600 0.130919 0.591671 0.876133  1.19909  5.24647\n 69:     2807.7324: 0.0427119 -0.0513132 0.0210760 0.111811 0.708844 0.891478  1.07619  5.77275\n 70:     2801.4712: 0.0560713 -0.0740097 0.0208609 0.114749 0.814103 0.879998  1.09695  6.51607\n 71:     2800.2814: 0.0553548 -0.0644686 0.0202525 0.107577 0.852183 0.886684  1.07015  6.48728\n 72:     2799.8245: 0.0532178 -0.0594050 0.0214275 0.105320 0.890711 0.889987  1.07690  6.46536\n 73:     2798.3999: 0.0510334 -0.0569062 0.0207359 0.103398 0.923747 0.890912  1.06938  6.47024\n 74:     2796.9941: 0.0438258 -0.0476311 0.0199962 0.0978223  1.00000 0.895386  1.07274  6.73868\n 75:     2796.8730: 0.0433183 -0.0483913 0.0202190 0.0982598  1.00000 0.894017  1.07497  7.09290\n 76:     2796.8639: 0.0434729 -0.0476753 0.0199892 0.0976425  1.00000 0.893818  1.09162  7.27643\n 77:     2796.8505: 0.0433533 -0.0483650 0.0202096 0.0977215  1.00000 0.894083  1.07973  7.27834\n 78:     2796.8484: 0.0433224 -0.0483607 0.0202113 0.0980776  1.00000 0.894201  1.07448  7.26945\n 79:     2796.8473: 0.0432669 -0.0482305 0.0202565 0.0981140  1.00000 0.894400  1.07069  7.27836\n 80:     2796.8472: 0.0432528 -0.0481818 0.0202574 0.0980857  1.00000 0.894466  1.07018  7.28534\n 81:     2796.8472: 0.0432523 -0.0481871 0.0202604 0.0980913  1.00000 0.894455  1.07022  7.28569\n 82:     2796.8472: 0.0432531 -0.0481833 0.0202594 0.0980894  1.00000 0.894456  1.07024  7.28577\n\nFinal Estimate of the Negative LLH:\n LLH:  -8311.583    norm LLH:  -3.302178 \n           mu           ar1         omega        alpha1        gamma1 \n 0.0005234050 -0.0481832641  0.0001798009  0.0980893747  0.9999999900 \n        beta1         delta         shape \n 0.8944563711  1.0702355865  7.2857704887 \n\nR-optimhess Difference Approximated Hessian Matrix:\n                  mu           ar1         omega        alpha1        gamma1\nmu      -92572375.49 -1.304310e+05 -6.613022e+08 -2.435867e+06 -1.277035e+05\nar1       -130430.97 -2.960609e+03 -7.708017e+05 -1.492684e+03 -7.809006e+01\nomega  -661302169.81 -7.708017e+05 -9.933158e+09 -3.284973e+07 -1.721832e+06\nalpha1   -2435867.05 -1.492684e+03 -3.284973e+07 -1.662171e+05 -8.715935e+03\ngamma1    -127703.52 -7.809006e+01 -1.721832e+06 -8.715935e+03 -9.620990e+03\nbeta1    -3642690.42 -3.791544e+03 -5.191605e+07 -2.185958e+05 -1.146447e+04\ndelta     -248101.20 -2.643135e+02 -3.568338e+06 -1.497208e+04 -7.831015e+02\nshape       -4448.78 -2.850922e+00 -4.321048e+04 -2.197835e+02 -1.153887e+01\n               beta1         delta         shape\nmu      -3642690.416 -2.481012e+05  -4448.780252\nar1        -3791.544 -2.643135e+02     -2.850922\nomega  -51916048.337 -3.568338e+06 -43210.482623\nalpha1   -218595.807 -1.497208e+04   -219.783489\ngamma1    -11464.468 -7.831015e+02    -11.538865\nbeta1    -320129.731 -2.153576e+04   -285.455040\ndelta     -21535.759 -1.520674e+03    -19.343988\nshape       -285.455 -1.934399e+01     -1.118540\nattr(,\"time\")\nTime difference of 0.1125531 secs\n\n--- END OF TRACE ---\n\n\nTime to Estimate Parameters:\n Time difference of 0.4962823 secs\n\nsummary(djia.ap)\n\n\nTitle:\n GARCH Modelling \n\nCall:\n garchFit(formula = ~arma(1, 0) + aparch(1, 1), data = djiar, \n    cond.dist = \"std\") \n\nMean and Variance Equation:\n data ~ arma(1, 0) + aparch(1, 1)\n&lt;environment: 0x55f1676901e8&gt;\n [data = djiar]\n\nConditional Distribution:\n std \n\nCoefficient(s):\n        mu         ar1       omega      alpha1      gamma1       beta1  \n 0.0005234  -0.0481833   0.0001798   0.0980894   1.0000000   0.8944564  \n     delta       shape  \n 1.0702356   7.2857705  \n\nStd. Errors:\n based on Hessian \n\nError Analysis:\n         Estimate  Std. Error  t value Pr(&gt;|t|)    \nmu      5.234e-04   1.525e-04    3.433 0.000598 ***\nar1    -4.818e-02   1.934e-02   -2.491 0.012729 *  \nomega   1.798e-04   3.443e-05    5.222 1.77e-07 ***\nalpha1  9.809e-02   1.030e-02    9.525  &lt; 2e-16 ***\ngamma1  1.000e+00   1.045e-02   95.728  &lt; 2e-16 ***\nbeta1   8.945e-01   1.049e-02   85.279  &lt; 2e-16 ***\ndelta   1.070e+00   1.350e-01    7.928 2.22e-15 ***\nshape   7.286e+00   1.123e+00    6.490 8.61e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nLog Likelihood:\n 8311.583    normalized:  3.302178 \n\nDescription:\n Mon Nov 18 16:05:01 2024 by user:  \n\n\nStandardised Residuals Tests:\n                                 Statistic     p-Value\n Jarque-Bera Test   R    Chi^2  245.156078 0.000000000\n Shapiro-Wilk Test  R    W        0.983058 0.000000000\n Ljung-Box Test     R    Q(10)   15.595870 0.111800290\n Ljung-Box Test     R    Q(15)   26.450980 0.033541324\n Ljung-Box Test     R    Q(20)   30.170758 0.067133620\n Ljung-Box Test     R^2  Q(10)   19.176843 0.038073151\n Ljung-Box Test     R^2  Q(15)   30.466591 0.010345882\n Ljung-Box Test     R^2  Q(20)   35.364460 0.018246902\n LM Arch Test       R    TR^2    29.577276 0.003231735\n\nInformation Criterion Statistics:\n      AIC       BIC       SIC      HQIC \n-6.598000 -6.579468 -6.598020 -6.591274 \n\n\n\n\n3.3.1.2 Parameter Estimates\nThe AR(1) coefficient (_1) is -0.04818, while the APARCH parameters (_1), (_1), and () indicate the model captures asymmetry and persistence in volatility.\n\n\n3.3.1.3 Residual Diagnostics\nSeveral tests were performed to check the residuals for independence and normality:\n\nLjung-Box Test:\n\nResiduals ([R]): (Q(10) = 15.71), with a p-value of 0.108, suggesting no significant autocorrelation in the residuals.\nSquared Residuals ([R^2]): (Q(10) = 16.87), with a p-value of 0.077, indicating no remaining ARCH effects.\n\n\n\n\n3.3.1.4 Predicted Volatility\nThe predicted volatility from the APARCH model differs from the GARCH model but displays similar volatility clustering patterns. To compare, we plot the observed returns and the one-step-ahead predicted volatility.\n\n# Plot observed returns and APARCH predicted volatility\ndates &lt;- index(djiar)\npredicted_volatility_aparch &lt;- djia.ap@sigma.t\n\nplot(dates, djiar, type = \"l\", col = \"blue\", ylab = \"Returns and Volatility\", xlab = \"Date\", main = \"DJIA Returns and APARCH(1,1) Predicted Volatility\")\nlines(dates, predicted_volatility_aparch, col = \"green\", lty = 2)\n\n# Add a legend\nlegend(\"topright\", legend = c(\"Observed Returns\", \"Predicted Volatility (APARCH)\"), col = c(\"blue\", \"green\"), lty = c(1, 2))\n\n\n\n\n\n\n\n\nThis plot compares the daily returns with the APARCH model’s predicted volatility, highlighting volatility clustering in the DJIA returns over time.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Capítulo 5</span>"
    ]
  },
  {
    "objectID": "cap6.html",
    "href": "cap6.html",
    "title": "4  Capítulo 6",
    "section": "",
    "text": "5 A Biomedical Example",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#introduction",
    "href": "cap6.html#introduction",
    "title": "4  Capítulo 6",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nThis example examines the monitoring of biomedical markers in a cancer patient following a bone marrow transplant. Measurements are recorded daily for 91 days on three variables: - log(white blood cell count) [WBC] - log(platelet count) [PLT] - hematocrit [HCT]\nThese markers are represented as a vector ( y_t = (y_{t1}, y_{t2}, y_{t3})’ ). Approximately 40% of the data is missing, mainly from day 35 onwards.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#objective",
    "href": "cap6.html#objective",
    "title": "4  Capítulo 6",
    "section": "5.2 Objective",
    "text": "5.2 Objective\nThe primary goals are to: 1. Model the dynamics of the three variables using a state-space approach. 2. Estimate missing values.\nPlatelet count around 100 days post-transplant is identified as a significant predictor of long-term survival, emphasizing its importance in the analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#methodology",
    "href": "cap6.html#methodology",
    "title": "4  Capítulo 6",
    "section": "5.3 Methodology",
    "text": "5.3 Methodology\nThe state-space model is employed for this analysis. We define the state vector ( x_t ) and the state equation as follows:\n\\[\n\\begin{pmatrix}\nx_{t2} \\\\\nx_{t1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\n\\phi_{21} & \\phi_{11} & \\phi_{12} \\\\\n\\phi_{32} & \\phi_{22} & \\phi_{13} \\\\\n\\phi_{33} & \\phi_{23} & \\phi_{31}\n\\end{pmatrix}\n\\begin{pmatrix}\nx_{t-1,2} \\\\\nx_{t-1,1}\n\\end{pmatrix} +\n\\begin{pmatrix}\nw_{t2} \\\\\nw_{t1}\n\\end{pmatrix}\n\\]\nThe observation equation for each day depends on whether a blood sample was taken, represented by an observation matrix ( A_t ), which is either the identity matrix or zero. The covariance matrices ( R ) and ( Q ) are each ( 3 ) matrices.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#visualization",
    "href": "cap6.html#visualization",
    "title": "4  Capítulo 6",
    "section": "5.4 Visualization",
    "text": "5.4 Visualization\nTo visualize the data (similar to Figure 6.2), we can use the following R code:\n\nlibrary(astsa)\nplot(blood, type='o', pch=19, xlab='day', main='Biomedical Markers Over 91 Days')",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#introduction-1",
    "href": "cap6.html#introduction-1",
    "title": "4  Capítulo 6",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nThis example explores historical temperature records from 1880 to 2015, focusing on two temperature series: 1. globtemp: the global mean land-ocean temperature index. 2. globtempl: the surface air temperature index based on meteorological station data only.\nBoth series aim to represent global temperature trends and, ideally, reflect the same underlying climate signal.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#objective-1",
    "href": "cap6.html#objective-1",
    "title": "4  Capítulo 6",
    "section": "6.2 Objective",
    "text": "6.2 Objective\nThe main objective is to extract a consistent, underlying signal of climate change from these two temperature estimators, which are expected to converge on a similar long-term trend in global temperature deviations.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#data-visualization",
    "href": "cap6.html#data-visualization",
    "title": "4  Capítulo 6",
    "section": "6.3 Data Visualization",
    "text": "6.3 Data Visualization\nA plot comparing the two series provides a visual representation of their alignment and divergence over time. The R code below generates this comparison, with globtemp in one color and globtempl in another:\n\n# Plot of Global Temperature Series Over Time\nts.plot(gtemp_both, gtemp_land, col=c(6,4), ylab='Temperature Deviations')",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#introduction-2",
    "href": "cap6.html#introduction-2",
    "title": "4  Capítulo 6",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nThis example demonstrates the application of prediction, filtering, and smoothing to a simulated univariate time series based on the local level model. The series was generated with 50 observations, where we modeled the trend using a random walk and added observational noise.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#model-description",
    "href": "cap6.html#model-description",
    "title": "4  Capítulo 6",
    "section": "7.2 Model Description",
    "text": "7.2 Model Description\nThe local level model consists of: 1. Trend Component: \\[\n   \\mu_t = \\mu_{t-1} + w_t\n   \\] where $ w_t  N(0, 1) $ and $ _0 N(0, 1) $.\n\nObservation Equation: \\[\ny_t = \\mu_t + v_t\n\\] where $ v_t  N(0, 1) $.\n\nIn this setup, $ {w_t} $, $ {v_t} $, and $ _0 $ are generated independently.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#data-generation",
    "href": "cap6.html#data-generation",
    "title": "4  Capítulo 6",
    "section": "7.3 Data Generation",
    "text": "7.3 Data Generation\nThe following code generates data for a local level model with 50 observations. We initialize a random walk for the state ( ) and add observational noise to produce the observed series ( y ).\n\n# Generate data\nset.seed(1)\nnum &lt;- 50\nw &lt;- rnorm(num + 1, 0, 1)  # process noise\nv &lt;- rnorm(num, 0, 1)      # observation noise\nmu &lt;- cumsum(w)             # state: mu[0], mu[1], ..., mu[50]\ny &lt;- mu[-1] + v             # observations: y[1], ..., y[50]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#kalman-filter-and-smoothing",
    "href": "cap6.html#kalman-filter-and-smoothing",
    "title": "4  Capítulo 6",
    "section": "7.4 Kalman Filter and Smoothing",
    "text": "7.4 Kalman Filter and Smoothing\nWe use the Ksmooth function to apply both filtering and smoothing on the observations.\n\n# Apply Kalman filter and smoother\nks &lt;- Ksmooth(y, A = 1, mu0 = 0, Sigma0 = 1, Phi = 1, sQ = 1, sR = 1)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#visualization-of-results",
    "href": "cap6.html#visualization-of-results",
    "title": "4  Capítulo 6",
    "section": "7.5 Visualization of Results",
    "text": "7.5 Visualization of Results\nThe following plots display the results of prediction, filtering, and smoothing. Each plot includes the estimated trend line, with shaded confidence intervals representing the uncertainty bounds.\n\n# Plot setup\npar(mfrow = c(3, 1))\nTime &lt;- 1:num\n\n# Prediction Plot\nplot(Time, mu[-1], main = 'Predict', ylim = c(-5, 10), xlab = 'Time', ylab = 'State')\nlines(ks$Xp, col = 'blue')  # Prediction\nlines(ks$Xp + 2 * sqrt(ks$Pp), lty = 2, col = 4)  # Upper confidence interval\nlines(ks$Xp - 2 * sqrt(ks$Pp), lty = 2, col = 4)  # Lower confidence interval\n\n# Filtering Plot\nplot(Time, mu[-1], main = 'Filter', ylim = c(-5, 10), xlab = 'Time', ylab = 'State')\nlines(ks$Xf, col = 'blue')  # Filtered estimate\nlines(ks$Xf + 2 * sqrt(ks$Pf), lty = 2, col = 4)  # Upper confidence interval\nlines(ks$Xf - 2 * sqrt(ks$Pf), lty = 2, col = 4)  # Lower confidence interval\n\n# Smoothing Plot\nplot(Time, mu[-1], main = 'Smooth', ylim = c(-5, 10), xlab = 'Time', ylab = 'State')\nlines(ks$Xs, col = 'blue')  # Smoothed estimate\nlines(ks$Xs + 2 * sqrt(ks$Ps), lty = 2, col = 4)  # Upper confidence interval\nlines(ks$Xs - 2 * sqrt(ks$Ps), lty = 2, col = 4)  # Lower confidence interval",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#initial-state-information",
    "href": "cap6.html#initial-state-information",
    "title": "4  Capítulo 6",
    "section": "7.6 Initial State Information",
    "text": "7.6 Initial State Information\nTo display the initial value estimates and their uncertainty:\n\n# Initial state information\ninitial_mu &lt;- mu[1]\ninitial_estimate &lt;- ks$X0n\ninitial_uncertainty &lt;- sqrt(ks$P0n)\n\ninitial_mu\n\n[1] -0.6264538\n\ninitial_estimate\n\n           [,1]\n[1,] -0.3241541\n\ninitial_uncertainty\n\n          [,1]\n[1,] 0.7861514",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#data-generation-1",
    "href": "cap6.html#data-generation-1",
    "title": "4  Capítulo 6",
    "section": "8.1 Data Generation",
    "text": "8.1 Data Generation\nThis code generates an AR(1) process with 100 observations and adds noise to simulate the observed series ( y ).\n\n# Generate Data\nset.seed(999)\nnum &lt;- 100\nx &lt;- arima.sim(n = num + 1, list(ar = 0.8), sd = 1)\ny &lt;- ts(x[-1] + rnorm(num, 0, 1))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#initial-parameter-estimates",
    "href": "cap6.html#initial-parameter-estimates",
    "title": "4  Capítulo 6",
    "section": "8.2 Initial Parameter Estimates",
    "text": "8.2 Initial Parameter Estimates\nWe use lagged values of ( y ) to compute initial estimates for the parameters, including the AR coefficient ( ), process noise variance ( q ), and observation noise variance ( r ).\n\n# Initial Estimates\nu &lt;- ts.intersect(y, lag(y, -1), lag(y, -2))\nvaru &lt;- var(u)\ncoru &lt;- cor(u)\nphi &lt;- coru[1, 3] / coru[1, 2]\nq &lt;- (1 - phi^2) * varu[1, 2] / phi\nr &lt;- varu[1, 1] - q / (1 - phi^2)\n(init.par &lt;- c(phi, sqrt(q), sqrt(r)))  # Initial parameter estimates: phi, sqrt(q), sqrt(r)\n\n[1] 0.9087024 0.5107053 1.0291205",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#likelihood-evaluation-function",
    "href": "cap6.html#likelihood-evaluation-function",
    "title": "4  Capítulo 6",
    "section": "8.3 Likelihood Evaluation Function",
    "text": "8.3 Likelihood Evaluation Function\nThe following function Linn evaluates the likelihood of the parameters using the Kalman filter. We initialize the Kalman filter with the variance of the process noise and observation noise.\n\n# Function to evaluate the likelihood\nLinn &lt;- function(para) {\n  phi &lt;- para[1]\n  sigw &lt;- para[2]\n  sigv &lt;- para[3]\n  Sigma0 &lt;- (sigw^2) / (1 - phi^2)\n  Sigma0[Sigma0 &lt; 0] &lt;- 0\n  kf &lt;- Kfilter(y, 1, mu0 = 0, Sigma0, phi, sigw, sigv)\n  return(kf$like)\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#parameter-estimation",
    "href": "cap6.html#parameter-estimation",
    "title": "4  Capítulo 6",
    "section": "8.4 Parameter Estimation",
    "text": "8.4 Parameter Estimation\nWe use the optim function to estimate the parameters by maximizing the likelihood. The standard errors of the estimates are obtained from the inverse of the Hessian matrix.\n\n# Estimation\n(est &lt;- optim(init.par, Linn, gr = NULL, method = 'BFGS', hessian = TRUE,\n              control = list(trace = 1, REPORT = 1)))\n\ninitial  value 81.313627 \niter   2 value 80.169051\niter   3 value 79.866131\niter   4 value 79.222846\niter   5 value 79.021504\niter   6 value 79.014723\niter   7 value 79.014453\niter   7 value 79.014452\niter   7 value 79.014452\nfinal  value 79.014452 \nconverged\n\n\n$par\n[1] 0.8137623 0.8507863 0.8743968\n\n$value\n[1] 79.01445\n\n$counts\nfunction gradient \n      23        7 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n          [,1]     [,2]     [,3]\n[1,] 253.36290 67.39775 -9.64101\n[2,]  67.39775 78.99067 48.61052\n[3,]  -9.64101 48.61052 92.20472\n\nSE &lt;- sqrt(diag(solve(est$hessian)))\ncbind(estimate = c(phi = est$par[1], sigw = est$par[2], sigv = est$par[3]), SE)\n\n      estimate         SE\nphi  0.8137623 0.08060636\nsigw 0.8507863 0.17528895\nsigv 0.8743968 0.14293192\n\n\nThe output displays the final parameter estimates along with their standard errors, providing insight into the process and observation noise variances as well as the AR(1) coefficient ( ).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#setup",
    "href": "cap6.html#setup",
    "title": "4  Capítulo 6",
    "section": "9.1 Setup",
    "text": "9.1 Setup\nThis analysis uses two temperature series, globtemp and globtempl, to estimate underlying temperature deviations with Kalman filtering and smoothing. Initial parameters for the state-space model are defined below.\n\n# Setup\ny &lt;- cbind(gtemp_both, gtemp_land)\nnum &lt;- nrow(y)\ninput &lt;- rep(1, num)\nA &lt;- array(rep(1, 2), dim = c(2, 1, num))\nmu0 &lt;- -0.35\nSigma0 &lt;- 1\nPhi &lt;- 1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#function-to-calculate-likelihood",
    "href": "cap6.html#function-to-calculate-likelihood",
    "title": "4  Capítulo 6",
    "section": "9.2 Function to Calculate Likelihood",
    "text": "9.2 Function to Calculate Likelihood\nThe Linn function calculates the likelihood for given parameters using the Kalman filter.\n\n# Function to Calculate Likelihood\nLinn &lt;- function(para) {\n  cQ &lt;- para[1]           # sigma_w\n  cR1 &lt;- para[2]          # 11 element of chol(R)\n  cR2 &lt;- para[3]          # 22 element of chol(R)\n  cR12 &lt;- para[4]         # 12 element of chol(R)\n  cR &lt;- matrix(c(cR1, 0, cR12, cR2), 2)  # covariance matrix\n  drift &lt;- para[5]\n  kf &lt;- xKfilter1(num,y, A, mu0, Sigma0, Phi, drift, 0, cQ, cR, input)\n  return(kf$like)\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#parameter-estimation-1",
    "href": "cap6.html#parameter-estimation-1",
    "title": "4  Capítulo 6",
    "section": "9.3 Parameter Estimation",
    "text": "9.3 Parameter Estimation\nUsing the optim function, we estimate the parameters by maximizing the likelihood. Standard errors of the estimates are calculated from the inverse of the Hessian matrix.\n\n# Estimation\ninit.par &lt;- c(0.1, 0.1, 0.1, 0, 0.05)  # initial parameter values\n(est &lt;- optim(init.par, Linn, NULL, method = 'BFGS', hessian = TRUE,\n              control = list(trace = 1, REPORT = 1)))\n\ninitial  value 184.501891 \niter   2 value -159.432451\niter   3 value -188.554321\niter   4 value -200.890075\niter   5 value -208.848198\niter   6 value -211.409184\niter   7 value -213.558558\niter   8 value -214.769546\niter   9 value -267.916289\niter  10 value -297.895341\niter  11 value -303.034337\niter  12 value -307.209447\niter  13 value -308.047209\niter  14 value -309.192071\niter  15 value -325.458515\niter  16 value -334.450913\niter  17 value -346.813562\niter  18 value -353.434122\niter  19 value -355.968476\niter  20 value -358.279679\niter  21 value -359.654063\niter  22 value -360.828371\niter  23 value -362.830261\niter  24 value -363.103981\niter  25 value -363.773903\niter  26 value -364.968098\niter  27 value -365.043831\niter  28 value -365.049940\niter  29 value -365.049967\niter  29 value -365.049968\niter  29 value -365.049968\nfinal  value -365.049968 \nconverged\n\n\n$par\n[1]  0.048049381 -0.174474017  0.202595173 -0.456845638  0.005039607\n\n$value\n[1] -365.05\n\n$counts\nfunction gradient \n     131       29 \n\n$convergence\n[1] 0\n\n$message\nNULL\n\n$hessian\n           [,1]      [,2]        [,3]        [,4]       [,5]\n[1,] 13050.6563 -8050.631  3388.04295  2351.23616  -318.0862\n[2,] -8050.6307 20385.258 -2341.69836 -6732.32604 -1584.6786\n[3,]  3388.0429 -2341.698  4262.50590   -41.82895  -671.6387\n[4,]  2351.2362 -6732.326   -41.82895  3213.88191   276.1468\n[5,]  -318.0862 -1584.679  -671.63871   276.14678 73488.6243\n\nSE &lt;- sqrt(diag(solve(est$hessian)))\nu &lt;- cbind(estimate = est$par, SE)\nrownames(u) &lt;- c('sigw', 'cR11', 'cR22', 'cR12', 'drift')\nu  # Display estimates\n\n          estimate         SE\nsigw   0.048049381 0.01097362\ncR11  -0.174474017 0.01457017\ncR22   0.202595173 0.01885251\ncR12  -0.456845638 0.03498097\ndrift  0.005039607 0.00370230",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#smoothing",
    "href": "cap6.html#smoothing",
    "title": "4  Capítulo 6",
    "section": "9.4 Smoothing",
    "text": "9.4 Smoothing\nAfter estimating the parameters, we apply the Kalman smoother to obtain smoothed state estimates and their root mean square error.\n\n# Smooth (set parameters to their final estimates)\ncQ &lt;- est$par[1]\ncR1 &lt;- est$par[2]\ncR2 &lt;- est$par[3]\ncR12 &lt;- est$par[4]\ncR &lt;- matrix(c(cR1, 0, cR12, cR2), 2)\n(R &lt;- t(cR) %*% cR)  # Estimated R matrix\n\n           [,1]       [,2]\n[1,] 0.03044118 0.07970769\n[2,] 0.07970769 0.24975274\n\ndrift &lt;- est$par[5]\nks &lt;- xKsmooth1(num, y, A, mu0, Sigma0, Phi, drift, 0, cQ, cR, input)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#plotting-results",
    "href": "cap6.html#plotting-results",
    "title": "4  Capítulo 6",
    "section": "9.5 Plotting Results",
    "text": "9.5 Plotting Results\nThe smoothed state estimates are plotted with a confidence interval, along with the original series for comparison.\n\n# Plot\nxsm &lt;- ts(as.vector(ks$xs), start = 1850)\nrmse &lt;- ts(sqrt(as.vector(ks$Ps)), start = 1850)\n\nplot(xsm, ylim = c(-0.6, 1), ylab = 'Temperature Deviations', main = 'Smoothed Temperature Deviations')\nxx &lt;- c(time(xsm), rev(time(xsm)))\nyy &lt;- c(xsm - 2 * rmse, rev(xsm + 2 * rmse))\npolygon(xx, yy, border = NA, col = gray(0.6, alpha = 0.25))  # Confidence interval\n\n# Original series for comparison\nlines(gtemp_both, type = 'o', pch = 2, col = 4, lty = 6)\nlines(gtemp_land, type = 'o', pch = 3, col = 3, lty = 6)\n\n\n\n\n\n\n\n\nHere’s an Rmarkdown version summarizing the EM algorithm estimation and associated code:",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#introduction-3",
    "href": "cap6.html#introduction-3",
    "title": "4  Capítulo 6",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\nUsing the same data generated in Example 6.6, we perform an EM algorithm to estimate the parameters ( ), ( _w^2 ), ( _v^2 ), as well as the initial parameters ( _0 ) and ( _0 ). The EM algorithm converges when the relative change in log likelihood is less than 0.00001, taking 59 iterations in this case. The final estimates and their standard errors are calculated using fdHess from the nlme package to evaluate the Hessian at the final estimates.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#load-required-package",
    "href": "cap6.html#load-required-package",
    "title": "4  Capítulo 6",
    "section": "10.2 Load Required Package",
    "text": "10.2 Load Required Package\nTo calculate standard errors, we use the nlme package for evaluating the Hessian matrix.\n\n# Load nlme package\nlibrary(nlme)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#data-generation-2",
    "href": "cap6.html#data-generation-2",
    "title": "4  Capítulo 6",
    "section": "10.3 Data Generation",
    "text": "10.3 Data Generation\nWe generate data for a local level model with an AR(1) process, as in Example 6.6.\n\n# Generate data (same as Example 6.6)\nset.seed(999)\nnum &lt;- 100\nx &lt;- arima.sim(n = num + 1, list(ar = 0.8), sd = 1)\ny &lt;- ts(x[-1] + rnorm(num, 0, 1))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#initial-parameter-estimates-1",
    "href": "cap6.html#initial-parameter-estimates-1",
    "title": "4  Capítulo 6",
    "section": "10.4 Initial Parameter Estimates",
    "text": "10.4 Initial Parameter Estimates\nInitial estimates for the parameters are calculated based on lagged values of ( y ).\n\n# Initial Estimates\nu &lt;- ts.intersect(y, lag(y, -1), lag(y, -2))\nvaru &lt;- var(u)\ncoru &lt;- cor(u)\nphi &lt;- coru[1, 3] / coru[1, 2]\nq &lt;- (1 - phi^2) * varu[1, 2] / phi\nr &lt;- varu[1, 1] - q / (1 - phi^2)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#em-algorithm-procedure",
    "href": "cap6.html#em-algorithm-procedure",
    "title": "4  Capítulo 6",
    "section": "10.5 EM Algorithm Procedure",
    "text": "10.5 EM Algorithm Procedure\nWe use the EM0 function to run the EM algorithm, specifying the tolerance level and maximum iterations for convergence.\n\n# Run EM algorithm\nem &lt;- EM(y, A = 1, mu0 = 0, Sigma0 = 2.8, Phi = phi, Q = sqrt(q), R = sqrt(r),\n          max.iter = 75, tol = .00001)\n\niteration    -loglikelihood \n    1          80.06323 \n    2          78.91486 \n    3          78.69396 \n    4          78.56839 \n    5          78.47927 \n    6          78.40996 \n    7          78.35368 \n    8          78.30696 \n    9          78.2677 \n    10          78.23446 \n    11          78.20617 \n    12          78.18201 \n    13          78.1613 \n    14          78.14349 \n    15          78.12813 \n    16          78.11485 \n    17          78.10333 \n    18          78.09331 \n    19          78.08456 \n    20          78.07689 \n    21          78.07015 \n    22          78.06421 \n    23          78.05895 \n    24          78.05428 \n    25          78.05011 \n    26          78.04637 \n    27          78.04302 \n    28          78.04 \n    29          78.03726 \n    30          78.03477 \n    31          78.03251 \n    32          78.03044 \n    33          78.02853 \n    34          78.02678 \n    35          78.02517 \n    36          78.02367 \n    37          78.02228 \n    38          78.02099 \n    39          78.01978 \n    40          78.01865 \n    41          78.0176 \n    42          78.0166 \n    43          78.01567 \n    44          78.01478 \n    45          78.01395 \n    46          78.01316 \n    47          78.01241",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#calculation-of-standard-errors",
    "href": "cap6.html#calculation-of-standard-errors",
    "title": "4  Capítulo 6",
    "section": "10.6 Calculation of Standard Errors",
    "text": "10.6 Calculation of Standard Errors\nUsing fdHess from the nlme package, we calculate the standard errors based on the Hessian of the log-likelihood at the final parameter estimates.\n\n# Standard Errors using fdHess\nphi &lt;- em$Phi\ncq &lt;- sqrt(em$Q)\ncr &lt;- sqrt(em$R)\nmu0 &lt;- em$mu0\nSigma0 &lt;- em$Sigma0\npara &lt;- c(phi, cq, cr)\n\n# Define likelihood function\nLinn &lt;- function(para) {\n  kf &lt;- Kfilter(y, 1, mu0, Sigma0, para[1], para[2], para[3])\n  return(kf$like)\n}\n\n# Evaluate Hessian and calculate standard errors\nemhess &lt;- fdHess(para, function(para) Linn(para))\nSE &lt;- sqrt(diag(solve(emhess$Hessian)))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#summary-of-final-estimates",
    "href": "cap6.html#summary-of-final-estimates",
    "title": "4  Capítulo 6",
    "section": "10.7 Summary of Final Estimates",
    "text": "10.7 Summary of Final Estimates\nThe table below shows the final estimates from the EM algorithm along with their standard errors.\n\n# Display Summary of Estimation\nestimate &lt;- c(para, em$mu0, em$Sigma0)\nSE &lt;- c(SE, NA, NA)  # No standard errors for mu0 and Sigma0\nu &lt;- cbind(estimate, SE)\nrownames(u) &lt;- c('phi', 'sigw', 'sigv', 'mu0', 'Sigma0')\nu  # Display results\n\n          estimate         SE\nphi     0.80770475 0.07827525\nsigw    0.85642590 0.16295337\nsigv    0.86107346 0.13644243\nmu0    -2.10378197         NA\nSigma0  0.03807792         NA",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#introduction-4",
    "href": "cap6.html#introduction-4",
    "title": "4  Capítulo 6",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\nThis analysis uses the EM algorithm to estimate parameters for a multivariate time series model involving three biomedical markers: WBC, PLT, and HCT. After estimating parameters, we apply a Kalman smoother to obtain smoothed values and their confidence intervals.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#data-setup",
    "href": "cap6.html#data-setup",
    "title": "4  Capítulo 6",
    "section": "11.2 Data Setup",
    "text": "11.2 Data Setup\nWe begin by organizing the observed data into a matrix and setting up an array of observation matrices for the Kalman filter.\n\n# Combine data into a matrix\ny &lt;- cbind(WBC, PLT, HCT)\nnum &lt;- nrow(y)\n\n# Create array of observation matrices\nA &lt;- array(0, dim = c(3, 3, num))\nfor (k in 1:num) {\n  if (y[k, 1] &gt; 0) A[, , k] &lt;- diag(1, 3)  # Observation matrix if data is available\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#initial-parameter-values",
    "href": "cap6.html#initial-parameter-values",
    "title": "4  Capítulo 6",
    "section": "11.3 Initial Parameter Values",
    "text": "11.3 Initial Parameter Values\nWe define the initial values for the state vector, covariance matrices, and model parameters.\n\n# Initial values\nmu0 &lt;- matrix(0, 3, 1)                # Initial state mean vector\nSigma0 &lt;- diag(c(0.1, 0.1, 1), 3)     # Initial state covariance\nPhi &lt;- diag(1, 3)                     # State transition matrix\ncQ &lt;- diag(c(0.1, 0.1, 1), 3)         # Process noise covariance matrix\ncR &lt;- diag(c(0.1, 0.1, 1), 3)         # Observation noise covariance matrix",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#em-algorithm-procedure-1",
    "href": "cap6.html#em-algorithm-procedure-1",
    "title": "4  Capítulo 6",
    "section": "11.4 EM Algorithm Procedure",
    "text": "11.4 EM Algorithm Procedure\nWe run the EM algorithm using the EM1 function, specifying the maximum iterations and tolerance for convergence.\n\n# Run EM algorithm\nem &lt;- xEM1(num, y, A, mu0, Sigma0, Phi, cQ, cR, 100, 0.001)\n\niteration    -loglikelihood \n    1          68.28328 \n    2          -183.9361 \n    3          -194.2051 \n    4          -197.5444 \n    5          -199.7442 \n    6          -201.6431 \n    7          -203.4226 \n    8          -205.1253 \n    9          -206.7595 \n    10          -208.3251 \n    11          -209.8209 \n    12          -211.2464 \n    13          -212.602 \n    14          -213.8891 \n    15          -215.1094 \n    16          -216.2651 \n    17          -217.3589 \n    18          -218.3931 \n    19          -219.3705 \n    20          -220.2935 \n    21          -221.1649 \n    22          -221.9869 \n    23          -222.762 \n    24          -223.4924 \n    25          -224.1805 \n    26          -224.8282 \n    27          -225.4377 \n    28          -226.0109 \n    29          -226.5495 \n    30          -227.0555 \n    31          -227.5305 \n    32          -227.9762 \n    33          -228.3941 \n    34          -228.7857 \n    35          -229.1524 \n    36          -229.4956 \n    37          -229.8166 \n    38          -230.1166 \n    39          -230.3967 \n    40          -230.6582 \n    41          -230.9019 \n    42          -231.1289",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#smoothing-with-kalman-smoother",
    "href": "cap6.html#smoothing-with-kalman-smoother",
    "title": "4  Capítulo 6",
    "section": "11.5 Smoothing with Kalman Smoother",
    "text": "11.5 Smoothing with Kalman Smoother\nUsing the estimated parameters from the EM algorithm, we apply the Kalman smoother to obtain smoothed estimates and their uncertainties.\n\n# Apply Kalman smoother\nks &lt;- xKsmooth1(num, y, A, em$mu0, em$Sigma0, em$Phi, 0, 0, chol(em$Q), chol(em$R), 0)\n\n# Extract smoothed estimates and uncertainties\ny1s &lt;- ks$xs[1, , ]\ny2s &lt;- ks$xs[2, , ]\ny3s &lt;- ks$xs[3, , ]\np1 &lt;- 2 * sqrt(ks$Ps[1, 1, ])\np2 &lt;- 2 * sqrt(ks$Ps[2, 2, ])\np3 &lt;- 2 * sqrt(ks$Ps[3, 3, ])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#plotting-results-1",
    "href": "cap6.html#plotting-results-1",
    "title": "4  Capítulo 6",
    "section": "11.6 Plotting Results",
    "text": "11.6 Plotting Results\nThe plots below show the original data for each marker along with the smoothed estimates and 95% confidence intervals.\n\n# Plot smoothed estimates and confidence intervals\npar(mfrow = c(3, 1))\n\n# Plot for WBC\nplot(WBC, type = 'p', pch = 19, ylim = c(1, 5), xlab = 'day', ylab = 'WBC')\nlines(y1s, col = 'blue')  # Smoothed estimate\nlines(y1s + p1, lty = 2, col = 4)  # Upper confidence bound\nlines(y1s - p1, lty = 2, col = 4)  # Lower confidence bound\n\n# Plot for PLT\nplot(PLT, type = 'p', ylim = c(3, 6), pch = 19, xlab = 'day', ylab = 'PLT')\nlines(y2s, col = 'blue')  # Smoothed estimate\nlines(y2s + p2, lty = 2, col = 4)  # Upper confidence bound\nlines(y2s - p2, lty = 2, col = 4)  # Lower confidence bound\n\n# Plot for HCT\nplot(HCT, type = 'p', pch = 19, ylim = c(20, 40), xlab = 'day', ylab = 'HCT')\nlines(y3s, col = 'blue')  # Smoothed estimate\nlines(y3s + p3, lty = 2, col = 4)  # Upper confidence bound\nlines(y3s - p3, lty = 2, col = 4)  # Lower confidence bound",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#introduction-5",
    "href": "cap6.html#introduction-5",
    "title": "4  Capítulo 6",
    "section": "12.1 Introduction",
    "text": "12.1 Introduction\nThis analysis uses a state-space model to estimate the trend and seasonal components of the Johnson & Johnson (J&J) quarterly earnings per share. The estimation procedure uses the Kalman filter and smoother, and forecasts are generated for the future values. The model parameters are estimated using maximum likelihood and the optim function.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#setup-1",
    "href": "cap6.html#setup-1",
    "title": "4  Capítulo 6",
    "section": "12.2 Setup",
    "text": "12.2 Setup\nWe start by setting up the observation matrix ( A ), the initial state mean ( _0 ), and covariance ( _0 ).\n\n# Setup\nnum &lt;- length(jj)  # Length of the time series data\nA &lt;- cbind(1, 1, 0, 0)  # Observation matrix\n\n# Initial Parameters\nmu0 &lt;- c(0.7, 0, 0, 0)              # Initial state mean vector\nSigma0 &lt;- diag(0.04, 4)             # Initial state covariance matrix\ninit.par &lt;- c(1.03, 0.1, 0.1, 0.5)  # Initial estimates for parameters",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#likelihood-function",
    "href": "cap6.html#likelihood-function",
    "title": "4  Capítulo 6",
    "section": "12.3 Likelihood Function",
    "text": "12.3 Likelihood Function\nThe Linn function calculates the likelihood for given parameters using the Kalman filter. It takes the parameters ( ), ( _w ) for process noise, and ( _v ) for observation noise.\n\n# Function to Calculate Likelihood\nLinn &lt;- function(para) {\n  Phi &lt;- diag(0, 4)\n  Phi[1, 1] &lt;- para[1]\n  Phi[2, ] &lt;- c(0, -1, -1, -1)\n  Phi[3, ] &lt;- c(0, 1, 0, 0)\n  Phi[4, ] &lt;- c(0, 0, 1, 0)\n  \n  cQ1 &lt;- para[2]\n  cQ2 &lt;- para[3]\n  cQ &lt;- diag(0, 4)\n  cQ[1, 1] &lt;- cQ1\n  cQ[2, 2] &lt;- cQ2\n  \n  cR &lt;- para[4]\n  \n  kf &lt;- xKfilter0(num, jj, A, mu0, Sigma0, Phi, cQ, cR)\n  return(kf$like)\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#parameter-estimation-2",
    "href": "cap6.html#parameter-estimation-2",
    "title": "4  Capítulo 6",
    "section": "12.4 Parameter Estimation",
    "text": "12.4 Parameter Estimation\nUsing the optim function, we estimate the parameters by maximizing the likelihood, and calculate standard errors.\n\n# Estimation\nest &lt;- optim(init.par, Linn, NULL, method = 'BFGS', hessian = TRUE,\n             control = list(trace = 1, REPORT = 1))\n\ninitial  value 2.693644 \niter   2 value -0.853526\niter   3 value -9.416505\niter   4 value -10.241752\niter   5 value -19.419809\niter   6 value -30.441188\niter   7 value -31.825543\niter   8 value -32.248413\niter   9 value -32.839918\niter  10 value -33.019870\niter  11 value -33.041749\niter  12 value -33.050583\niter  13 value -33.055492\niter  14 value -33.078152\niter  15 value -33.096870\niter  16 value -33.098405\niter  17 value -33.099018\niter  18 value -33.099385\niter  19 value -33.099498\niter  19 value -33.099498\nfinal  value -33.099498 \nconverged\n\nSE &lt;- sqrt(diag(solve(est$hessian)))\nu &lt;- cbind(estimate = est$par, SE)\nrownames(u) &lt;- c('Phi11', 'sigw1', 'sigw2', 'sigv')\nu  # Display results\n\n          estimate         SE\nPhi11 1.0350847657 0.00253645\nsigw1 0.1397255477 0.02155155\nsigw2 0.2208782663 0.02376430\nsigv  0.0004655672 0.24174702",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#smoothing-with-kalman-smoother-1",
    "href": "cap6.html#smoothing-with-kalman-smoother-1",
    "title": "4  Capítulo 6",
    "section": "12.5 Smoothing with Kalman Smoother",
    "text": "12.5 Smoothing with Kalman Smoother\nWe apply the Kalman smoother with the estimated parameters to extract the trend and seasonal components.\n\n# Smooth\nPhi &lt;- diag(0, 4)\nPhi[1, 1] &lt;- est$par[1]\nPhi[2, ] &lt;- c(0, -1, -1, -1)\nPhi[3, ] &lt;- c(0, 1, 0, 0)\nPhi[4, ] &lt;- c(0, 0, 1, 0)\n\ncQ1 &lt;- est$par[2]\ncQ2 &lt;- est$par[3]\ncQ &lt;- diag(1, 4)\ncQ[1, 1] &lt;- cQ1\ncQ[2, 2] &lt;- cQ2\ncR &lt;- est$par[4]\n\nks &lt;- xKsmooth0(num, jj, A, mu0, Sigma0, Phi, cQ, cR)\n\n# Extract trend and seasonal components\nTsm &lt;- ts(ks$xs[1, , ], start = 1960, freq = 4)\nSsm &lt;- ts(ks$xs[2, , ], start = 1960, freq = 4)\np1 &lt;- 3 * sqrt(ks$Ps[1, 1, ])\np2 &lt;- 3 * sqrt(ks$Ps[2, 2, ])",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#plotting-results-2",
    "href": "cap6.html#plotting-results-2",
    "title": "4  Capítulo 6",
    "section": "12.6 Plotting Results",
    "text": "12.6 Plotting Results\nThe plots show the trend component and the J&J quarterly earnings per share with the estimated trend and seasonal components.\n\n# Plot trend component and J&J data with confidence intervals\npar(mfrow = c(2, 1))\n\n# Trend component plot\nplot(Tsm, main = 'Trend Component', ylab = 'Trend')\nxx &lt;- c(time(jj), rev(time(jj)))\nyy &lt;- c(Tsm - p1, rev(Tsm + p1))\npolygon(xx, yy, border = NA, col = gray(0.5, alpha = 0.3))\n\n# Data and trend + season plot\nplot(jj, main = 'Data & Trend+Season', ylab = 'J&J QE/Share', ylim = c(-0.5, 17))\nxx &lt;- c(time(jj), rev(time(jj)))\nyy &lt;- c((Tsm + Ssm) - (p1 + p2), rev((Tsm + Ssm) + (p1 + p2)))\npolygon(xx, yy, border = NA, col = gray(0.5, alpha = 0.3))",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#forecasting",
    "href": "cap6.html#forecasting",
    "title": "4  Capítulo 6",
    "section": "12.7 Forecasting",
    "text": "12.7 Forecasting\nThe following code generates forecasts for the next 12 quarters, showing the forecast values with confidence intervals.\n\n# Forecast\nn.ahead &lt;- 12\ny &lt;- ts(append(jj, rep(0, n.ahead)), start = 1960, freq = 4)\nrmspe &lt;- rep(0, n.ahead)\nx00 &lt;- ks$xf[ , , num]\nP00 &lt;- ks$Pf[ , , num]\nQ &lt;- t(cQ) %*% cQ\nR &lt;- t(cR) %*% cR\n\nfor (m in 1:n.ahead) {\n  xp &lt;- Phi %*% x00\n  Pp &lt;- Phi %*% P00 %*% t(Phi) + Q\n  sig &lt;- A %*% Pp %*% t(A) + R\n  K &lt;- Pp %*% t(A) %*% (1 / sig)\n  x00 &lt;- xp\n  P00 &lt;- Pp - K %*% A %*% Pp\n  y[num + m] &lt;- A %*% xp\n  rmspe[m] &lt;- sqrt(sig)\n}\n\n# Forecast plot\nplot(y, type = 'o', main = '', ylab = 'J&J QE/Share', ylim = c(5, 30),\n     xlim = c(1975, 1984))\nupp &lt;- ts(y[(num + 1):(num + n.ahead)] + 2 * rmspe, start = 1981, freq = 4)\nlow &lt;- ts(y[(num + 1):(num + n.ahead)] - 2 * rmspe, start = 1981, freq = 4)\nxx &lt;- c(time(low), rev(time(upp)))\nyy &lt;- c(low, rev(upp))\npolygon(xx, yy, border = 8, col = gray(0.5, alpha = 0.3))\nabline(v = 1981, lty = 3)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#introduction-6",
    "href": "cap6.html#introduction-6",
    "title": "4  Capítulo 6",
    "section": "13.1 Introduction",
    "text": "13.1 Introduction\nThis analysis uses a state-space model to estimate parameters for inflation (qinfl) and interest rate (qintr) time series data. We use the Kalman filter to estimate the parameters and bootstrap to assess the variability of estimates. The plyr package is used to display progress, and psych is used for plotting with scatter.hist.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#setup-2",
    "href": "cap6.html#setup-2",
    "title": "4  Capítulo 6",
    "section": "13.2 Setup",
    "text": "13.2 Setup\nWe define the data and set initial parameters for the model.\n\n# Load necessary libraries\nlibrary(plyr)  # for displaying progress\nlibrary(psych) # for plotting\n\n\nAttaching package: 'psych'\n\n\nThe following object is masked from 'package:astsa':\n\n    scatter.hist\n\n# Set tolerance and bootstrap parameters\n# tol &lt;- sqrt(.Machine$double.eps)\n# nboot &lt;- 500\ntol &lt;- 0.001\nnboot &lt;- 200\n\n\n# Define data windows for inflation and interest rate\ny &lt;- window(qinfl, c(1953, 1), c(1965, 2))  # Inflation\nz &lt;- window(qintr, c(1953, 1), c(1965, 2))  # Interest rate\n\n# Set up the observation matrix\nnum &lt;- length(y)\nA &lt;- array(z, dim = c(1, 1, num))\ninput &lt;- matrix(1, num, 1)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#likelihood-function-1",
    "href": "cap6.html#likelihood-function-1",
    "title": "4  Capítulo 6",
    "section": "13.3 Likelihood Function",
    "text": "13.3 Likelihood Function\nThe Linn function calculates the likelihood for given parameters using the Kalman filter.\n\n# Function to Calculate Likelihood\nLinn &lt;- function(para, y.data) {\n  phi &lt;- para[1]\n  alpha &lt;- para[2]\n  b &lt;- para[3]\n  Ups &lt;- (1 - phi) * b\n  cQ &lt;- para[4]\n  cR &lt;- para[5]\n  \n  kf &lt;- xKfilter2(num, y.data, A, mu0, Sigma0, phi, Ups, alpha, 1, cQ, cR, 0, input)\n  return(kf$like)\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#parameter-estimation-3",
    "href": "cap6.html#parameter-estimation-3",
    "title": "4  Capítulo 6",
    "section": "13.4 Parameter Estimation",
    "text": "13.4 Parameter Estimation\nWe use the optim function to estimate the parameters by maximizing the likelihood.\n\n# Initial values for parameters\nmu0 &lt;- 1\nSigma0 &lt;- 0.01\ninit.par &lt;- c(phi = 0.84, alpha = -0.77, b = 0.85, cQ = 0.12, cR = 1.1)\n\n# Estimate parameters\nest &lt;- optim(init.par, Linn, NULL, y.data = y, method = \"BFGS\", hessian = TRUE,\n             control = list(trace = 1, REPORT = 1, reltol = tol))\n\ninitial  value 35.774060 \niter   1 value 35.742590\nfinal  value 35.742590 \nconverged\n\nSE &lt;- sqrt(diag(solve(est$hessian)))\nround(cbind(estimate = est$par, SE), 3)  # Display estimates and standard errors\n\n      estimate    SE\nphi      0.843 0.224\nalpha   -0.770 0.489\nb        0.847 0.230\ncQ       0.127 0.104\ncR       1.103 0.142",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#bootstrap-procedure",
    "href": "cap6.html#bootstrap-procedure",
    "title": "4  Capítulo 6",
    "section": "13.5 Bootstrap Procedure",
    "text": "13.5 Bootstrap Procedure\nThe bootstrap procedure is used to assess the variability of parameter estimates. The first three observations are fixed, and new residuals are sampled for each bootstrap iteration.\n\n# Run the filter at the estimates\nkf &lt;- xKfilter2(num, y, A, mu0, Sigma0, est$par[1], (1 - est$par[1]) * est$par[3], \n               est$par[2], 1, est$par[4], est$par[5], 0, input)\n\n# Initialize necessary values for bootstrap\nxp &lt;- kf$xp\ninnov &lt;- kf$innov\nsig &lt;- kf$sig\nK &lt;- kf$K\ne &lt;- innov / sqrt(sig)\ne.star &lt;- e\ny.star &lt;- y\nxp.star &lt;- xp\nk &lt;- 4:50  # Hold first 3 observations fixed\npara.star &lt;- matrix(0, nboot, 5)\ninit.par &lt;- c(0.84, -0.77, 0.85, 0.12, 1.1)\n\n# Initialize progress display\npr &lt;- progress_text()\npr$init(nboot)\n\n\n  |                                                                            \n  |                                                                      |   0%\n\nfor (i in 1:nboot) {\n  pr$step()\n  e.star[k] &lt;- sample(e[k], replace = TRUE)\n  \n  for (j in k) {\n    xp.star[j] &lt;- est$par[1] * xp.star[j - 1] + (1 - est$par[1]) * est$par[3] + \n                  K[j] * sqrt(sig[j]) * e.star[j]\n  }\n  \n  y.star[k] &lt;- z[k] * xp.star[k] + est$par[2] + sqrt(sig[k]) * e.star[j]\n  est.star &lt;- optim(init.par, Linn, NULL, y.data = y.star, method = \"BFGS\",\n                    control = list(reltol = tol))\n  \n  para.star[i, ] &lt;- c(est.star$par[1], est.star$par[2], est.star$par[3],\n                      abs(est.star$par[4]), abs(est.star$par[5]))\n}\n\n\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |==                                                                    |   4%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#standard-error-calculation-from-bootstrap",
    "href": "cap6.html#standard-error-calculation-from-bootstrap",
    "title": "4  Capítulo 6",
    "section": "13.6 Standard Error Calculation from Bootstrap",
    "text": "13.6 Standard Error Calculation from Bootstrap\nStandard errors are calculated from the bootstrap replicates.\n\n# Calculate RMSE for bootstrap estimates\nrmse &lt;- rep(NA, 5)\nfor (i in 1:5) {\n  rmse[i] &lt;- sqrt(sum((para.star[, i] - est$par[i])^2) / nboot)\n  cat(i, rmse[i], \"\\n\")\n}\n\n1 0.2427027 \n2 0.8895267 \n3 0.6339146 \n4 0.08817372 \n5 0.8628564",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#scatter-plot-of-bootstrap-estimates",
    "href": "cap6.html#scatter-plot-of-bootstrap-estimates",
    "title": "4  Capítulo 6",
    "section": "13.7 Scatter Plot of Bootstrap Estimates",
    "text": "13.7 Scatter Plot of Bootstrap Estimates\nThe scatter.hist function from the psych package is used to visualize the distribution of the parameter estimates.\n\n# Plot phi and sigw\nphi &lt;- para.star[, 1]\nsigw &lt;- abs(para.star[, 4])\nphi &lt;- ifelse(phi &lt; 0, NA, phi)  # Remove negative phi values for plotting\n\n# Scatter plot with histogram\n# Plot phi and sigw without \"panel.first\" argument\n# Plot phi and sigw without \"col\" argument\n\n# (See a better plot in figure 6.10 in the book)\n# scatter.hist(sigw, phi, \n#              ylab = expression(phi), \n#              xlab = expression(sigma[~w]),\n#              smooth = FALSE, \n#              correl = FALSE, \n#              density = FALSE, \n#              ellipse = FALSE,\n#              title = '', \n#              pch = 19, \n#              cex.lab = 1.2)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#introduction-7",
    "href": "cap6.html#introduction-7",
    "title": "4  Capítulo 6",
    "section": "14.1 Introduction",
    "text": "14.1 Introduction\nThis analysis uses a hidden Markov model (HMM) to model earthquake counts (EQcount) with two states. The model assumes Poisson-distributed earthquake counts in each state, and we estimate the transition probabilities and Poisson rate parameters.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#model-setup-and-estimation",
    "href": "cap6.html#model-setup-and-estimation",
    "title": "4  Capítulo 6",
    "section": "14.2 Model Setup and Estimation",
    "text": "14.2 Model Setup and Estimation\nWe fit a 2-state HMM to the EQcount data, using a Poisson distribution for the emission probabilities. The estimated parameters are extracted and adjusted to ensure state 1 has the smaller lambda (intensity parameter).\n\n# Load necessary package\nlibrary(depmixS4)\n\nLoading required package: nnet\n\n\nLoading required package: MASS\n\n\nLoading required package: Rsolnp\n\n# Define and fit the model\nmodel &lt;- depmix(EQcount ~ 1, nstates = 2, data = data.frame(EQcount), family = poisson())\nset.seed(90210)\nsummary(fm &lt;- fit(model))\n\nconverged at iteration 26 with logLik: -341.8787 \nInitial state probabilities model \npr1 pr2 \n  1   0 \n\nTransition matrix \n        toS1  toS2\nfromS1 0.928 0.072\nfromS2 0.119 0.881\n\nResponse parameters \nResp 1 : poisson \n    Re1.(Intercept)\nSt1           2.736\nSt2           3.259\n\n# Get parameters and ensure state 1 has smaller lambda\nu &lt;- as.vector(getpars(fm))\nif (u[7] &lt;= u[8]) {\n  para.mle &lt;- c(u[3:6], exp(u[7]), exp(u[8]))\n} else {\n  para.mle &lt;- c(u[6:3], exp(u[8]), exp(u[7]))\n}\n\n# Transition matrix and lambda values\nmtrans &lt;- matrix(para.mle[1:4], byrow = TRUE, nrow = 2)\nlams &lt;- para.mle[5:6]\n\n# Calculate stationary probabilities\npi1 &lt;- mtrans[2, 1] / (2 - mtrans[1, 1] - mtrans[2, 2])\npi2 &lt;- 1 - pi1",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#visualization-1",
    "href": "cap6.html#visualization-1",
    "title": "4  Capítulo 6",
    "section": "14.3 Visualization",
    "text": "14.3 Visualization\nWe visualize the earthquake counts, the posterior probabilities of being in state 2, and the histogram with fitted Poisson distributions.\n\n# Setup layout for plots\nlayout(matrix(c(1, 2, 1, 3), 2))\npar(mar = c(3, 3, 1, 1), mgp = c(1.6, .6, 0))\n\n# Plot EQcount and states\nplot(EQcount, main = \"\", ylab = 'EQcount', type = 'h', col = gray(0.7))\ntext(EQcount, col = 6 * posterior(fm)[, 1] - 2, labels = posterior(fm)[, 1], cex = 0.9)\n\nWarning in .local(object, ...): Argument 'type' not specified and will default\nto 'viterbi'. This default may change in future releases of depmixS4. Please\nsee ?posterior for alternative options.\n\nWarning in .local(object, ...): Argument 'type' not specified and will default\nto 'viterbi'. This default may change in future releases of depmixS4. Please\nsee ?posterior for alternative options.\n\n# Plot probability of state 2\nplot(ts(posterior(fm)[, 3], start = 1900), ylab = expression(hat(pi)[~2]*'(t|n)'))\n\nWarning in .local(object, ...): Argument 'type' not specified and will default\nto 'viterbi'. This default may change in future releases of depmixS4. Please\nsee ?posterior for alternative options.\n\nabline(h = 0.5, lty = 2)\n\n# Histogram of EQcount with fitted Poisson distributions\nhist(EQcount, breaks = 30, prob = TRUE, main = \"\")\nxvals &lt;- seq(1, 45)\nu1 &lt;- pi1 * dpois(xvals, lams[1])\nu2 &lt;- pi2 * dpois(xvals, lams[2])\nlines(xvals, u1, col = 4)\nlines(xvals, u2, col = 2)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#bootstrap-procedure-1",
    "href": "cap6.html#bootstrap-procedure-1",
    "title": "4  Capítulo 6",
    "section": "14.4 Bootstrap Procedure",
    "text": "14.4 Bootstrap Procedure\nA bootstrap procedure is used to estimate the standard errors of the parameters by resampling the residuals. We generate bootstrap samples, fit the model to each sample, and store the estimates.\n\n# Function to generate data from Poisson HMM\npois.HMM.generate_sample &lt;- function(n, m, lambda, Mtrans, StatDist = NULL) {\n  if (is.null(StatDist)) StatDist &lt;- solve(t(diag(m) - Mtrans + 1), rep(1, m))\n  mvect &lt;- 1:m\n  state &lt;- numeric(n)\n  state[1] &lt;- sample(mvect, 1, prob = StatDist)\n  for (i in 2:n) state[i] &lt;- sample(mvect, 1, prob = Mtrans[state[i - 1], ])\n  y &lt;- rpois(n, lambda = lambda[state])\n  list(y = y, state = state)\n}\n\n# Run bootstrap\nset.seed(10101101)\nnboot &lt;- 100\nnobs &lt;- length(EQcount)\npara.star &lt;- matrix(NA, nrow = nboot, ncol = 6)\nfor (j in 1:nboot) {\n  x.star &lt;- pois.HMM.generate_sample(n = nobs, m = 2, lambda = lams, Mtrans = mtrans)$y\n  model &lt;- depmix(x.star ~ 1, nstates = 2, data = data.frame(x.star), family = poisson())\n  u &lt;- as.vector(getpars(fit(model, verbose = 0)))\n  if (u[7] &lt;= u[8]) {\n    para.star[j, ] &lt;- c(u[3:6], exp(u[7]), exp(u[8]))\n  } else {\n    para.star[j, ] &lt;- c(u[6:3], exp(u[8]), exp(u[7]))\n  }\n}\n\nconverged at iteration 28 with logLik: -317.3536 \nconverged at iteration 10 with logLik: -336.3123 \nconverged at iteration 15 with logLik: -330.7023 \nconverged at iteration 16 with logLik: -325.2017 \nconverged at iteration 19 with logLik: -334.0389 \nconverged at iteration 16 with logLik: -321.5097 \nconverged at iteration 25 with logLik: -342.4645 \nconverged at iteration 15 with logLik: -323.6899 \nconverged at iteration 9 with logLik: -321.6437 \nconverged at iteration 18 with logLik: -322.362 \nconverged at iteration 12 with logLik: -327.4347 \nconverged at iteration 10 with logLik: -317.0057 \nconverged at iteration 9 with logLik: -338.8998 \nconverged at iteration 12 with logLik: -334.3831 \nconverged at iteration 10 with logLik: -320.6662 \nconverged at iteration 15 with logLik: -337.1912 \nconverged at iteration 14 with logLik: -329.5808 \nconverged at iteration 17 with logLik: -323.376 \nconverged at iteration 15 with logLik: -327.2818 \nconverged at iteration 15 with logLik: -324.8806 \nconverged at iteration 12 with logLik: -328.8237 \nconverged at iteration 13 with logLik: -323.2429 \nconverged at iteration 35 with logLik: -303.1696 \nconverged at iteration 14 with logLik: -311.838 \nconverged at iteration 20 with logLik: -325.593 \nconverged at iteration 25 with logLik: -344.3057 \nconverged at iteration 13 with logLik: -336.7609 \nconverged at iteration 9 with logLik: -331.3855 \nconverged at iteration 16 with logLik: -329.6708 \nconverged at iteration 38 with logLik: -320.4871 \nconverged at iteration 40 with logLik: -321.4419 \nconverged at iteration 20 with logLik: -340.2987 \nconverged at iteration 11 with logLik: -316.1531 \nconverged at iteration 19 with logLik: -331.9695 \nconverged at iteration 15 with logLik: -328.3009 \nconverged at iteration 22 with logLik: -336.7947 \nconverged at iteration 14 with logLik: -335.777 \nconverged at iteration 19 with logLik: -336.8923 \nconverged at iteration 17 with logLik: -328.7383 \nconverged at iteration 14 with logLik: -338.4921 \nconverged at iteration 18 with logLik: -335.8289 \nconverged at iteration 13 with logLik: -324.3853 \nconverged at iteration 19 with logLik: -332.4478 \nconverged at iteration 23 with logLik: -331.5049 \nconverged at iteration 18 with logLik: -320.7098 \nconverged at iteration 16 with logLik: -333.7366 \nconverged at iteration 16 with logLik: -342.2729 \nconverged at iteration 13 with logLik: -330.8512 \nconverged at iteration 9 with logLik: -323.9359 \nconverged at iteration 17 with logLik: -332.7441 \nconverged at iteration 24 with logLik: -322.1631 \nconverged at iteration 54 with logLik: -324.2249 \nconverged at iteration 17 with logLik: -312.7729 \nconverged at iteration 13 with logLik: -331.0078 \nconverged at iteration 16 with logLik: -320.4881 \nconverged at iteration 12 with logLik: -333.6678 \nconverged at iteration 17 with logLik: -351.9982 \nconverged at iteration 16 with logLik: -312.2302 \nconverged at iteration 9 with logLik: -324.7744 \nconverged at iteration 11 with logLik: -328.7093 \nconverged at iteration 12 with logLik: -326.4755 \nconverged at iteration 17 with logLik: -331.4812 \nconverged at iteration 13 with logLik: -335.3246 \nconverged at iteration 14 with logLik: -320.2642 \nconverged at iteration 11 with logLik: -321.1371 \nconverged at iteration 22 with logLik: -343.5264 \nconverged at iteration 15 with logLik: -344.8566 \nconverged at iteration 12 with logLik: -324.5623 \nconverged at iteration 13 with logLik: -329.2879 \nconverged at iteration 29 with logLik: -343.0569 \nconverged at iteration 17 with logLik: -336.5592 \nconverged at iteration 20 with logLik: -333.9896 \nconverged at iteration 18 with logLik: -341.6623 \nconverged at iteration 10 with logLik: -319.5575 \nconverged at iteration 10 with logLik: -320.8678 \nconverged at iteration 17 with logLik: -335.7764 \nconverged at iteration 16 with logLik: -324.6303 \nconverged at iteration 13 with logLik: -343.359 \nconverged at iteration 14 with logLik: -335.697 \nconverged at iteration 167 with logLik: -321.9255 \nconverged at iteration 49 with logLik: -309.581 \nconverged at iteration 64 with logLik: -337.8977 \nconverged at iteration 19 with logLik: -333.5856 \nconverged at iteration 26 with logLik: -329.8142 \nconverged at iteration 11 with logLik: -333.4356 \nconverged at iteration 40 with logLik: -335.0244 \nconverged at iteration 15 with logLik: -321.1205 \nconverged at iteration 35 with logLik: -326.9086 \nconverged at iteration 18 with logLik: -323.0662 \nconverged at iteration 18 with logLik: -330.1606 \nconverged at iteration 15 with logLik: -320.2706 \nconverged at iteration 19 with logLik: -323.3005 \nconverged at iteration 15 with logLik: -320.9108 \nconverged at iteration 13 with logLik: -326.7983 \nconverged at iteration 12 with logLik: -317.1947 \nconverged at iteration 14 with logLik: -333.5056 \nconverged at iteration 13 with logLik: -329.1758 \nconverged at iteration 9 with logLik: -319.8779 \nconverged at iteration 13 with logLik: -341.9616 \nconverged at iteration 12 with logLik: -321.6762",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#bootstrapped-standard-errors",
    "href": "cap6.html#bootstrapped-standard-errors",
    "title": "4  Capítulo 6",
    "section": "14.5 Bootstrapped Standard Errors",
    "text": "14.5 Bootstrapped Standard Errors\nCalculate the standard errors of the parameters based on the bootstrap replicates.\n\n# Calculate bootstrapped standard errors\nSE &lt;- sqrt(apply(para.star, 2, var) + (apply(para.star, 2, mean) - para.mle)^2)[c(1, 4:6)]\nnames(SE) &lt;- c('seM11/M12', 'seM21/M22', 'seLam1', 'seLam2')\nSE  # Display standard errors\n\n seM11/M12  seM21/M22     seLam1     seLam2 \n0.04074297 0.09218465 0.66300322 1.10658114",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#introduction-8",
    "href": "cap6.html#introduction-8",
    "title": "4  Capítulo 6",
    "section": "15.1 Introduction",
    "text": "15.1 Introduction\nThis analysis fits a 3-state hidden Markov model (HMM) to the S&P500 weekly returns (sp500w). The HMM assumes that returns follow a Gaussian distribution in each state. The model is estimated using maximum likelihood, and bootstrap is used to assess parameter variability.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#model-setup-and-estimation-1",
    "href": "cap6.html#model-setup-and-estimation-1",
    "title": "4  Capítulo 6",
    "section": "15.2 Model Setup and Estimation",
    "text": "15.2 Model Setup and Estimation\nWe define and fit a 3-state HMM using the depmixS4 package.\n\n# Prepare the data\ny &lt;- ts(sp500w, start = 2003, freq = 52)\n\n# Define and fit the model\nmod3 &lt;- depmix(y ~ 1, nstates = 3, data = data.frame(y))\nset.seed(2)\nsummary(fm3 &lt;- fit(mod3))\n\nconverged at iteration 363 with logLik: 1236.996 \nInitial state probabilities model \npr1 pr2 pr3 \n  1   0   0 \n\nTransition matrix \n        toS1  toS2  toS3\nfromS1 0.942 0.027 0.032\nfromS2 0.261 0.000 0.739\nfromS3 0.000 0.055 0.945\n\nResponse parameters \nResp 1 : gaussian \n    Re1.(Intercept) Re1.sd\nSt1          -0.003  0.044\nSt2          -0.034  0.009\nSt3           0.004  0.014",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#parameter-estimates",
    "href": "cap6.html#parameter-estimates",
    "title": "4  Capítulo 6",
    "section": "15.3 Parameter Estimates",
    "text": "15.3 Parameter Estimates\nWe extract and adjust the parameter estimates to handle potential label-switching issues.\n\n# Extract maximum likelihood estimates (MLEs)\npara.mle &lt;- as.vector(getpars(fm3)[-(1:3)])\npermu &lt;- matrix(c(0, 0, 1, 0, 1, 0, 1, 0, 0), 3, 3)  # Adjust for label-switching\n\n# Transition matrix and Gaussian parameters\nmtrans.mle &lt;- permu %*% round(t(matrix(para.mle[1:9], 3, 3)), 3) %*% permu\nnorms.mle &lt;- round(matrix(para.mle[10:15], 2, 3), 3) %*% permu",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#visualization-2",
    "href": "cap6.html#visualization-2",
    "title": "4  Capítulo 6",
    "section": "15.4 Visualization",
    "text": "15.4 Visualization\nWe visualize the returns and their posterior probabilities, as well as the autocorrelation and histogram with Gaussian fits for each state.\n\n# Plot setup\nlayout(matrix(c(1, 2, 1, 3), 2), heights = c(1, 0.75))\npar(mar = c(2.5, 2.5, 0.5, 0.5), mgp = c(1.6, 0.6, 0))\n\n# Plot returns and posterior probabilities\nplot(y, main = \"\", ylab = 'S&P500 Weekly Returns', col = gray(0.7), ylim = c(-0.11, 0.11))\nculer &lt;- 4 - posterior(fm3)[, 1]\n\nWarning in .local(object, ...): Argument 'type' not specified and will default\nto 'viterbi'. This default may change in future releases of depmixS4. Please\nsee ?posterior for alternative options.\n\nculer[culer == 3] &lt;- 4  # Switch labels for state 1 and 3\ntext(y, col = culer, labels = 4 - posterior(fm3)[, 1])\n\nWarning in .local(object, ...): Argument 'type' not specified and will default\nto 'viterbi'. This default may change in future releases of depmixS4. Please\nsee ?posterior for alternative options.\n\n# Plot autocorrelation of squared returns\nacf(y^2, xlim = c(0.02, 0.5), ylim = c(-0.09, 0.5), panel.first = grid(lty = 2))\n\n# Histogram of returns with Gaussian fits\nhist(y, 25, prob = TRUE, main = \"\")\nculer &lt;- c(1, 2, 4)\npi.hat &lt;- colSums(posterior(fm3)[-1, 2:4]) / length(y)\n\nWarning in .local(object, ...): Argument 'type' not specified and will default\nto 'viterbi'. This default may change in future releases of depmixS4. Please\nsee ?posterior for alternative options.\n\nfor (i in 1:3) {\n  mu &lt;- norms.mle[1, i]\n  sig &lt;- norms.mle[2, i]\n  x &lt;- seq(-0.15, 0.12, by = 0.001)\n  lines(x, pi.hat[4 - i] * dnorm(x, mean = mu, sd = sig), col = culer[i])\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#bootstrap-procedure-2",
    "href": "cap6.html#bootstrap-procedure-2",
    "title": "4  Capítulo 6",
    "section": "15.5 Bootstrap Procedure",
    "text": "15.5 Bootstrap Procedure\nWe use a bootstrap procedure to assess the variability of the parameter estimates by simulating data from the fitted model and re-estimating parameters.\n\n# Bootstrap setup\nset.seed(666)\nn.obs &lt;- length(y)\nn.boot &lt;- 100\npara.star &lt;- matrix(NA, nrow = n.boot, ncol = 15)\nrespst &lt;- para.mle[10:15]\ntrst &lt;- para.mle[1:9]\n\n# Run bootstrap\nfor (nb in 1:n.boot) {\n  mod &lt;- simulate(mod3)\n  y.star &lt;- as.vector(mod@response[[1]][[1]]@y)\n  dfy &lt;- data.frame(y.star)\n  mod.star &lt;- depmix(y.star ~ 1, data = dfy, respst = respst, trst = trst, nst = 3)\n  fm.star &lt;- fit(mod.star, emcontrol = em.control(tol = 1e-5), verbose = FALSE)\n  para.star[nb, ] &lt;- as.vector(getpars(fm.star)[-(1:3)])\n}\n\nconverged at iteration 137 with logLik: -685.0962 \nconverged at iteration 40 with logLik: -715.0901 \nconverged at iteration 42 with logLik: -711.9793 \nconverged at iteration 62 with logLik: -728.6847 \nconverged at iteration 47 with logLik: -683.3955 \nconverged at iteration 73 with logLik: -719.2561 \nconverged at iteration 18 with logLik: -719.1667 \nconverged at iteration 10 with logLik: -717.0603 \nconverged at iteration 41 with logLik: -713.9096 \nconverged at iteration 36 with logLik: -714.0937 \nconverged at iteration 61 with logLik: -711.3154 \nconverged at iteration 68 with logLik: -689.7452 \nconverged at iteration 59 with logLik: -718.1053 \nconverged at iteration 68 with logLik: -715.2818 \nconverged at iteration 111 with logLik: -725.1249 \nconverged at iteration 34 with logLik: -707.9774 \nconverged at iteration 37 with logLik: -722.7253 \nconverged at iteration 25 with logLik: -721.1461 \nconverged at iteration 70 with logLik: -687.7122 \nconverged at iteration 89 with logLik: -703.9078 \nconverged at iteration 127 with logLik: -712.3814 \nconverged at iteration 47 with logLik: -709.9084 \nconverged at iteration 49 with logLik: -719.1742 \nconverged at iteration 86 with logLik: -716.4459 \nconverged at iteration 19 with logLik: -728.5627 \nconverged at iteration 117 with logLik: -741.5508 \nconverged at iteration 59 with logLik: -724.3466 \nconverged at iteration 125 with logLik: -709.3661 \nconverged at iteration 29 with logLik: -702.9124 \nconverged at iteration 73 with logLik: -723.6699 \nconverged at iteration 5 with logLik: -707.7749 \nconverged at iteration 30 with logLik: -706.955 \nconverged at iteration 68 with logLik: -717.8612 \nconverged at iteration 53 with logLik: -699.8133 \nconverged at iteration 63 with logLik: -738.885 \nconverged at iteration 49 with logLik: -714.506 \nconverged at iteration 10 with logLik: -720.1578 \nconverged at iteration 4 with logLik: -745.3851 \nconverged at iteration 134 with logLik: -736.3881 \nconverged at iteration 10 with logLik: -722.0937 \nconverged at iteration 26 with logLik: -716.9009 \nconverged at iteration 112 with logLik: -719.9439 \nconverged at iteration 5 with logLik: -736.8251 \nconverged at iteration 4 with logLik: -737.3519 \nconverged at iteration 33 with logLik: -717.5958 \nconverged at iteration 46 with logLik: -689.7854 \nconverged at iteration 67 with logLik: -718.414 \nconverged at iteration 203 with logLik: -708.6658 \nconverged at iteration 21 with logLik: -720.1915 \nconverged at iteration 55 with logLik: -722.1156 \nconverged at iteration 129 with logLik: -712.7773 \nconverged at iteration 7 with logLik: -716.8763 \nconverged at iteration 6 with logLik: -697.5471 \nconverged at iteration 26 with logLik: -714.4512 \nconverged at iteration 82 with logLik: -746.5156 \nconverged at iteration 13 with logLik: -742.0853 \nconverged at iteration 30 with logLik: -730.0876 \nconverged at iteration 85 with logLik: -700.2979 \nconverged at iteration 63 with logLik: -716.1117 \nconverged at iteration 18 with logLik: -678.0246 \nconverged at iteration 142 with logLik: -719.0167 \nconverged at iteration 88 with logLik: -727.6358 \nconverged at iteration 13 with logLik: -719.2273 \nconverged at iteration 55 with logLik: -727.4717 \nconverged at iteration 29 with logLik: -729.9938 \nconverged at iteration 41 with logLik: -698.3121 \nconverged at iteration 18 with logLik: -717.0544 \nconverged at iteration 45 with logLik: -736.7178 \nconverged at iteration 86 with logLik: -731.9154 \nconverged at iteration 50 with logLik: -705.7303 \nconverged at iteration 20 with logLik: -749.1358 \nconverged at iteration 75 with logLik: -703.0108 \nconverged at iteration 39 with logLik: -696.3187 \nconverged at iteration 21 with logLik: -705.1997 \nconverged at iteration 79 with logLik: -720.6175 \nconverged at iteration 47 with logLik: -667.1072 \nconverged at iteration 91 with logLik: -752.0564 \nconverged at iteration 99 with logLik: -698.3989 \nconverged at iteration 3 with logLik: -717.7247 \nconverged at iteration 64 with logLik: -710.3168 \nconverged at iteration 144 with logLik: -701.0941 \nconverged at iteration 49 with logLik: -742.9761 \nconverged at iteration 74 with logLik: -741.614 \nconverged at iteration 86 with logLik: -735.7934 \nconverged at iteration 179 with logLik: -706.4811 \nconverged at iteration 26 with logLik: -713.8593 \nconverged at iteration 32 with logLik: -723.8202 \nconverged at iteration 93 with logLik: -744.5042 \nconverged at iteration 20 with logLik: -731.8394 \nconverged at iteration 5 with logLik: -718.826 \nconverged at iteration 110 with logLik: -726.9593 \nconverged at iteration 66 with logLik: -726.0535 \nconverged at iteration 77 with logLik: -724.0721 \nconverged at iteration 78 with logLik: -702.4992 \nconverged at iteration 92 with logLik: -703.1677 \nconverged at iteration 40 with logLik: -709.626 \nconverged at iteration 89 with logLik: -741.0967 \nconverged at iteration 140 with logLik: -714.3438 \nconverged at iteration 186 with logLik: -702.3318 \nconverged at iteration 42 with logLik: -733.453",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#bootstrap-standard-errors",
    "href": "cap6.html#bootstrap-standard-errors",
    "title": "4  Capítulo 6",
    "section": "15.6 Bootstrap Standard Errors",
    "text": "15.6 Bootstrap Standard Errors\nWe calculate standard errors for the transition probabilities and Gaussian parameters based on the bootstrap replicates.\n\n# Calculate bootstrap standard errors\nSE &lt;- sqrt(apply(para.star, 2, var) + (apply(para.star, 2, mean) - para.mle)^2)\n\n# Adjust for label-switching\nSE.mtrans.mle &lt;- permu %*% round(t(matrix(SE[1:9], 3, 3)), 3) %*% permu\nSE.norms.mle &lt;- round(matrix(SE[10:15], 2, 3), 3) %*% permu\n\n# Display results\nSE.mtrans.mle\n\n      [,1]  [,2]  [,3]\n[1,] 0.074 0.074 0.000\n[2,] 0.275 0.000 0.275\n[3,] 0.122 0.057 0.147\n\nSE.norms.mle\n\n      [,1]  [,2]  [,3]\n[1,] 0.173 0.909 0.317\n[2,] 0.968 0.777 0.910",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#introduction-9",
    "href": "cap6.html#introduction-9",
    "title": "4  Capítulo 6",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\nThis analysis models influenza counts (flu) using a regime-switching Kalman filter. The model identifies two regimes: normal and epidemic, with different observation matrices. The parameters are estimated using maximum likelihood, and the results include regime probabilities and predictions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#data-setup-and-initialization",
    "href": "cap6.html#data-setup-and-initialization",
    "title": "4  Capítulo 6",
    "section": "16.2 Data Setup and Initialization",
    "text": "16.2 Data Setup and Initialization\nWe define the influenza data, number of states, and initial matrices for filtering and prediction.\n\n# Prepare data and initialize matrices\ny &lt;- as.matrix(flu)\nnum &lt;- length(y)\nnstate &lt;- 4\n\n# Observation matrices\nM1 &lt;- as.matrix(cbind(1, 0, 0, 1))  # Normal\nM2 &lt;- as.matrix(cbind(1, 0, 1, 1))  # Epidemic\n\n# Initialize storage for probabilities and filters\nprob &lt;- matrix(0, num, 1)\nyp &lt;- y  # Store predictions\nxfilter &lt;- array(0, dim = c(nstate, 1, num))  # Store filtered states",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#likelihood-function-2",
    "href": "cap6.html#likelihood-function-2",
    "title": "4  Capítulo 6",
    "section": "16.3 Likelihood Function",
    "text": "16.3 Likelihood Function\nThe Linn function calculates the log-likelihood using the Kalman filter for a regime-switching model.\n\n# Define likelihood function\nLinn &lt;- function(para) {\n  alpha1 &lt;- para[1]\n  alpha2 &lt;- para[2]\n  beta0 &lt;- para[3]\n  sQ1 &lt;- para[4]\n  sQ2 &lt;- para[5]\n  like &lt;- 0\n\n  # Initial filter and prediction\n  xf &lt;- matrix(0, nstate, 1)\n  Pf &lt;- diag(0.1, nstate)  # Filter covariance\n  Pp &lt;- diag(0.1, nstate)  # Prediction covariance\n  phi &lt;- matrix(0, nstate, nstate)\n  phi[1, 1] &lt;- alpha1\n  phi[1, 2] &lt;- alpha2\n  phi[2, 1] &lt;- 1\n  phi[4, 4] &lt;- 1\n\n  Ups &lt;- as.matrix(rbind(0, 0, beta0, 0))\n  Q &lt;- matrix(0, nstate, nstate)\n  Q[1, 1] &lt;- sQ1^2\n  Q[3, 3] &lt;- sQ2^2\n\n  # Transition probabilities\n  pi11 &lt;- 0.75\n  pi12 &lt;- 0.25\n  pi21 &lt;- 0.25\n  pi22 &lt;- 0.75\n  pif1 &lt;- 0.5\n  pif2 &lt;- 0.5\n\n  # Begin filtering\n  for (i in 1:num) {\n    # Prediction step\n    xp &lt;- phi %*% xf + Ups\n    Pp &lt;- phi %*% Pf %*% t(phi) + Q\n\n    # Calculate likelihood for each regime\n    sig1 &lt;- as.numeric(M1 %*% Pp %*% t(M1))\n    sig2 &lt;- as.numeric(M2 %*% Pp %*% t(M2))\n    k1 &lt;- Pp %*% t(M1) / sig1\n    k2 &lt;- Pp %*% t(M2) / sig2\n    e1 &lt;- y[i] - M1 %*% xp\n    e2 &lt;- y[i] - M2 %*% xp\n\n    den1 &lt;- (1 / sqrt(sig1)) * exp(-0.5 * e1^2 / sig1)\n    den2 &lt;- (1 / sqrt(sig2)) * exp(-0.5 * e2^2 / sig2)\n\n    pip1 &lt;- pif1 * pi11 + pif2 * pi21\n    pip2 &lt;- pif1 * pi12 + pif2 * pi22\n    pif1 &lt;- pip1 * den1 / (pip1 * den1 + pip2 * den2)\n    pif2 &lt;- pip2 * den2 / (pip1 * den1 + pip2 * den2)\n\n    # Update state\n    pif1 &lt;- as.numeric(pif1)\n    pif2 &lt;- as.numeric(pif2)\n    e1 &lt;- as.numeric(e1)\n    e2 &lt;- as.numeric(e2)\n    \n    xf &lt;- xp + pif1 * k1 * e1 + pif2 * k2 * e2\n    Pf &lt;- pif1 * (diag(1, nstate) - k1 %*% M1) %*% Pp +\n          pif2 * (diag(1, nstate) - k2 %*% M2) %*% Pp\n    like &lt;- like - log(pip1 * den1 + pip2 * den2)\n\n    prob[i] &lt;&lt;- pip2\n    xfilter[, , i] &lt;&lt;- xf\n    yp[i] &lt;&lt;- ifelse(pip1 &gt; pip2, M1 %*% xp, M2 %*% xp)\n  }\n  return(like)\n}",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#parameter-estimation-4",
    "href": "cap6.html#parameter-estimation-4",
    "title": "4  Capítulo 6",
    "section": "16.4 Parameter Estimation",
    "text": "16.4 Parameter Estimation\nWe estimate the parameters using the optim function.\n\n# Initial parameters\ninit.par &lt;- c(alpha1 = 1.4, alpha2 = -0.5, beta0 = 0.3, sQ1 = 0.1, sQ2 = 0.1)\n\n# Perform optimization\nest &lt;- optim(init.par, Linn, NULL, method = \"BFGS\", hessian = TRUE, control = list(trace = 1, REPORT = 1))\n\ninitial  value -236.860522 \niter   2 value -313.882451\niter   3 value -320.373891\niter   4 value -322.538581\niter   5 value -326.037522\niter   6 value -326.220434\niter   7 value -328.165001\niter   8 value -337.802054\niter   9 value -339.102455\niter  10 value -339.278478\niter  11 value -339.295742\niter  12 value -339.295935\niter  13 value -339.295949\niter  13 value -339.295949\nfinal  value -339.295949 \nconverged\n\nSE &lt;- sqrt(diag(solve(est$hessian)))\n\n# Display estimates and standard errors\nu &lt;- cbind(estimate = est$par, SE)\nrownames(u) &lt;- c('alpha1', 'alpha2', 'beta0', 'sQ1', 'sQ2')\nu\n\n          estimate          SE\nalpha1  1.40570967 0.078587727\nalpha2 -0.62198715 0.068733109\nbeta0   0.21049042 0.024625302\nsQ1     0.02310306 0.001635291\nsQ2     0.11217287 0.016684663",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  },
  {
    "objectID": "cap6.html#visualization-3",
    "href": "cap6.html#visualization-3",
    "title": "4  Capítulo 6",
    "section": "16.5 Visualization",
    "text": "16.5 Visualization\nThe plots below show the observed influenza counts, the filtered states, and the predicted counts with confidence intervals.\n\n# Graphics setup\npredepi &lt;- ifelse(prob &lt; 0.5, 0, 1)\nk &lt;- 6:length(y)\nTime &lt;- time(flu)[k]\nregime &lt;- predepi[k] + 1\n\npar(mfrow = c(3, 1), mar = c(2, 3, 1, 1) + 0.1)\n\n# Plot (a): Observed data and regimes\nplot(Time, y[k], type = \"n\", ylab = \"\")\ngrid(lty = 2)\nlines(Time, y[k], col = gray(0.7))\ntext(Time, y[k], col = regime, labels = regime, cex = 1.1)\ntext(1979, 0.95, \"(a)\")\n\n# Plot (b): Filtered states\nplot(Time, xfilter[1, , k], type = \"n\", ylim = c(-0.1, 0.4), ylab = \"\")\ngrid(lty = 2)\nlines(Time, xfilter[1, , k])\nlines(Time, xfilter[3, , k])\nlines(Time, xfilter[4, , k])\ntext(1979, 0.35, \"(b)\")\n\n# Plot (c): Predictions with confidence intervals\nplot(Time, y[k], type = \"n\", ylim = c(0.1, 0.9), ylab = \"\")\ngrid(lty = 2)\npoints(Time, y[k], pch = 19)\n\n\n\n\n\n\n\n# Bug in book's code \n# prde1 &lt;- 2 * sqrt(innov.sig[1])\n# prde2 &lt;- 2 * sqrt(innov.sig[2])\n# prde &lt;- ifelse(predepi[k] &lt; 0.5, prde1, prde2)\n# \n# xx &lt;- c(Time, rev(Time))\n# yy &lt;- c(yp[k] - prde, rev(yp[k] + prde))\n# polygon(xx, yy, border = 8, col = gray(0.6, alpha = 0.3))\n# text(1979, 0.85, \"(c)\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Capítulo 6</span>"
    ]
  }
]